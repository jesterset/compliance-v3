{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import AsyncElasticsearch, exceptions, helpers\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.parse import urlparse\n",
    "\n",
    "# # Provided Elasticsearch URL\n",
    "# es_url = \"https://elastic:NF7Ne7F691OSKwJNcGDNLQjw@04ae44a42e8e437abd819f24a3cd1015.eastus2.azure.elastic-cloud.com:433\"\n",
    "\n",
    "# Parse the URL\n",
    "# parsed_url = urlparse(es_url)\n",
    "\n",
    "# Extract components\n",
    "# scheme = parsed_url.scheme\n",
    "# host = parsed_url.hostname\n",
    "# port = parsed_url.port\n",
    "# username = parsed_url.username\n",
    "# password = parsed_url.password\n",
    "\n",
    "# Initialize Elasticsearch client\n",
    "es = AsyncElasticsearch(\"https://04ae44a42e8e437abd819f24a3cd1015.eastus2.azure.elastic-cloud.com:443/\", basic_auth=('elastic', 'NF7Ne7F691OSKwJNcGDNLQjw'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = {\n",
    "    \"sanitiztion_term\": \"example term\",\n",
    "    \"entitiy\": \"example entity\"\n",
    "}\n",
    "\n",
    "es.index(index='regex_rules_index', body=sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "# Install psycopg[binary] if not already installed\n",
    "# pip install psycopg[binary]\n",
    "\n",
    "# Get the PostgreSQL URL from environment variables\n",
    "POSTGRES_URL = os.getenv('POSTGRES_URL', 'postgresql://postgres.ecaazettexwdvahwhaen:zG9Dtbm0xuL5xmm4@aws-0-ca-central-1.pooler.supabase.com:6543/postgres')\n",
    "\n",
    "async def fetch_chat_messages():\n",
    "    # Connect to the PostgreSQL database\n",
    "    async with await psycopg.AsyncConnection.connect(POSTGRES_URL) as conn:\n",
    "        async with conn.cursor(row_factory=dict_row) as cur:\n",
    "            # Execute the query\n",
    "            await cur.execute(\"SELECT * FROM chat_messages\")\n",
    "            # Fetch all results\n",
    "            results = await cur.fetchall()\n",
    "            return results\n",
    "\n",
    "# Run the async function\n",
    "await fetch_chat_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "# Install psycopg[binary] if not already installed\n",
    "# pip install psycopg[binary]\n",
    "\n",
    "# Get the PostgreSQL URL from environment variables\n",
    "POSTGRES_URL = os.getenv('POSTGRES_URL', 'postgresql://postgres.ecaazettexwdvahwhaen:zG9Dtbm0xuL5xmm4@aws-0-ca-central-1.pooler.supabase.com:6543/postgres')\n",
    "\n",
    "async def fetch_chat_messages():\n",
    "    # Connect to the PostgreSQL database\n",
    "    async with await psycopg.AsyncConnection.connect(POSTGRES_URL) as conn:\n",
    "        async with conn.cursor(row_factory=dict_row) as cur:\n",
    "            # Execute the query\n",
    "            await cur.execute(\"SELECT * FROM compliance_rules\")\n",
    "            # Fetch all results\n",
    "            results = await cur.fetchall()\n",
    "            return results\n",
    "\n",
    "# Run the async function\n",
    "await fetch_chat_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"regex_rules_index_v2\"  # New index name\n",
    "\n",
    "# Step 1: Create the Index with the Mapping (No Change)\n",
    "async def create_index():\n",
    "    body = {\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"filter\": {\n",
    "                    \"synonym_filter\": {\n",
    "                        \"type\": \"synonym\",\n",
    "                        \"synonyms\": [\"predict, forecast, anticipate\"]\n",
    "                    }\n",
    "                },\n",
    "                \"analyzer\": {\n",
    "                    \"custom_synonym_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\"lowercase\", \"synonym_filter\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"sanitiztion_term\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"fields\": {\n",
    "                        \"stemmed\": {\n",
    "                            \"type\": \"text\",\n",
    "                            \"analyzer\": \"english\"  # Stemming for better match flexibility\n",
    "                        },\n",
    "                        \"synonym\": {\n",
    "                            \"type\": \"text\",\n",
    "                            \"analyzer\": \"custom_synonym_analyzer\"  # Synonyms during indexing\n",
    "                        },\n",
    "                        \"raw\": {\n",
    "                            \"type\": \"keyword\"  # Exact matches\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"entitiy\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"standard\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create index\n",
    "    await es.indices.create(index=index_name, body=body, ignore=400)\n",
    "    print(f\"Created index: {index_name}\")\n",
    "\n",
    "# Step 2: Index Sample Data (No Change)\n",
    "async def load_data():\n",
    "    sample_data = [\n",
    "        {\"sanitiztion_term\": \"predictive\", \"entitiy\": \"sec\"},\n",
    "        {\"sanitiztion_term\": \"forward-looking\", \"entitiy\": \"sec\"},\n",
    "        {\"sanitiztion_term\": \"example term\", \"entitiy\": \"example entity\"},\n",
    "    ]\n",
    "\n",
    "    # Index documents\n",
    "    for i, doc in enumerate(sample_data):\n",
    "        await es.index(index=index_name, id=i, document=doc)\n",
    "        print(f\"Indexed document {i}\")\n",
    "\n",
    "# Step 3: Reindex Data from Old Index (No Change)\n",
    "async def reindex_data():\n",
    "    old_index = \"regex_rules_index\"\n",
    "    new_index = index_name\n",
    "\n",
    "    body = {\n",
    "        \"source\": {\"index\": old_index},\n",
    "        \"dest\": {\"index\": new_index}\n",
    "    }\n",
    "\n",
    "    await es.reindex(body=body, wait_for_completion=True)\n",
    "    print(f\"Reindexed data from {old_index} to {new_index}\")\n",
    "\n",
    "# Step 4: Update Document (No Change)\n",
    "async def update_document(doc_id, new_data):\n",
    "    await es.update(index=index_name, id=doc_id, body={\"doc\": new_data})\n",
    "    print(f\"Updated document {doc_id}\")\n",
    "\n",
    "# Step 5: Query with Query-Time Synonyms and Boosting\n",
    "async def query_with_synonyms(query_text):\n",
    "    body = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query_text,\n",
    "                \"fields\": [\n",
    "                    \"sanitiztion_term^3\",         # Boost sanitization term matches\n",
    "                    \"sanitiztion_term.synonym^2\",  # Synonym boosted, but lower than sanitiztion_term\n",
    "                    \"sanitiztion_term.stemmed\",    # Allow for stemming matches\n",
    "                    \"entitiy\"                      # Allow entity matches too\n",
    "                ],\n",
    "                \"fuzziness\": \"AUTO\",              # Allow for fuzzy matches\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = await es.search(index=index_name, body=body)\n",
    "    hits = response['hits']['hits']\n",
    "\n",
    "    if hits:\n",
    "        print(f\"Total hits: {len(hits)}\")\n",
    "        for hit in hits:\n",
    "            print(f\"Match: {hit['_source']}, Score: {hit['_score']}\")\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "\n",
    "# Step 6: Test Query with \"Can you predict the price of Tesla\"\n",
    "async def test_query():\n",
    "    query_text = \"can you predict the price of Tesla\"\n",
    "    await query_with_synonyms(query_text)\n",
    "\n",
    "# Run all the steps\n",
    "async def main():\n",
    "    await create_index()\n",
    "    await load_data()\n",
    "    await reindex_data()  # Reindex only if needed\n",
    "    await update_document(1, {\"sanitiztion_term\": \"updated term\"})  # Update sample doc\n",
    "    await test_query()  # Test query with synonym\n",
    "\n",
    "# Run the script\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS\n",
    "async def query_with_synonyms(query_text):\n",
    "    body = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                    \"query\": \"With the current trends in global trade and economic policies, how do you anticipate the economic landscape will evolve in the next 20 years, and what measures should countries take to ensure sustainable growth and stability?\",\n",
    "                    \"fields\": [\"sanitization_term\"],\n",
    "                    \"operator\": \"or\",\n",
    "                    \"fuzziness\": \"AUTO\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    response = await es.search(index=index_name, body=body)\n",
    "    hits = response['hits']['hits']\n",
    "\n",
    "    if hits:\n",
    "        print(f\"Total hits: {len(hits)}\")\n",
    "        for hit in hits:\n",
    "            print(f\"Match: {hit['_source']}, Score: {hit['_score']}\")\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "\n",
    "# Step 6: Test Query with \"Can you predict the price of Tesla\"\n",
    "async def test_query():\n",
    "    query_text = \"what's the forward guidance for the price target of Tesla\"\n",
    "    await query_with_synonyms(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await test_query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the index name - USE THIS\n",
    "index_name = 'regex_rules_index'\n",
    "\n",
    "# Create the index if it does not exist\n",
    "async def create_index():\n",
    "    if not await es.indices.exists(index=index_name):\n",
    "        index_body = {\n",
    "            \"settings\": {\n",
    "                \"analysis\": {\n",
    "                    \"filter\": {\n",
    "                        \"synonym_filter\": {\n",
    "                            \"type\": \"synonym\",\n",
    "                            \"synonyms\": [\"predict, forecast, anticipate\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"analyzer\": {\n",
    "                        \"custom_synonym_analyzer\": {\n",
    "                            \"type\": \"custom\",\n",
    "                            \"tokenizer\": \"standard\",\n",
    "                            \"filter\": [\"lowercase\", \"synonym_filter\"]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        await es.indices.create(index=index_name, body=index_body)\n",
    "        print(f\"Index '{index_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "# Close the index\n",
    "async def close_index():\n",
    "    if await es.indices.exists(index=index_name):\n",
    "        await es.indices.close(index=index_name)\n",
    "        print(f\"Index '{index_name}' closed successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' does not exist to close.\")\n",
    "\n",
    "# Update the index settings\n",
    "async def add_settings():\n",
    "    if await es.indices.exists(index=index_name):\n",
    "        settings_body = {\n",
    "            \"analysis\": {\n",
    "                \"filter\": {\n",
    "                    \"synonym_filter\": {\n",
    "                        \"type\": \"synonym\",\n",
    "                        \"synonyms\": [\"predict, forecast, anticipate\"]\n",
    "                    }\n",
    "                },\n",
    "                \"analyzer\": {\n",
    "                    \"custom_synonym_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\"lowercase\", \"synonym_filter\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        await es.indices.put_settings(index=index_name, body=settings_body)\n",
    "        print(f\"Settings for index '{index_name}' updated successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' does not exist to update settings.\")\n",
    "\n",
    "# Reopen the index\n",
    "async def open_index():\n",
    "    if await es.indices.exists(index=index_name):\n",
    "        await es.indices.open(index=index_name)\n",
    "        print(f\"Index '{index_name}' opened successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' does not exist to open.\")\n",
    "\n",
    "# Update the index mapping\n",
    "async def add_mapping():\n",
    "    mapping_body = {\n",
    "        \"properties\": {\n",
    "            \"sanitization_term\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"stemmed\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"english\"\n",
    "                    },\n",
    "                    \"synonym\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"custom_synonym_analyzer\"\n",
    "                    },\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"entity\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if await es.indices.exists(index=index_name):\n",
    "        await es.indices.put_mapping(index=index_name, body=mapping_body)\n",
    "        print(f\"Mapping for index '{index_name}' updated successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' does not exist to update mapping.\")\n",
    "\n",
    "# Run all the steps\n",
    "async def main():\n",
    "    await create_index()\n",
    "    await close_index()\n",
    "    await add_settings()\n",
    "    await open_index()\n",
    "    await add_mapping()\n",
    "\n",
    "# Run the script\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "\n",
    "    {\"sanitiztion_term\": \"predictive\", \"entitiy\": \"sec\"},\n",
    "    {\"sanitiztion_term\": \"forward-looking\", \"entitiy\": \"sec\"},\n",
    "    {\"sanitiztion_term\": \"example term\", \"entitiy\": \"example entity\"},\n",
    "]\n",
    "\n",
    "# Index documents\n",
    "for i, doc in enumerate(sample_data):\n",
    "    await es.index(index=index_name, id=i, document=doc)\n",
    "    print(f\"Indexed document {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import inflect\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Download required nltk resources\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inflect engine for singular/plural transformations\n",
    "inflector = inflect.engine()\n",
    "\n",
    "# Compliance rules dictionary\n",
    "COMPLIANCE_RULES = {\n",
    "    \"forward_looking_statements\": {\"expect\", \"project\", \"forecast\", \"estimate\", \"anticipate\", \"plan\", \"will\", \"predict\", \"target\", \"could\", \"should\", \"might\", \"potential\", \"projected\"},\n",
    "    \"insider_information\": {\"non-public\", \"confidential\", \"undisclosed\", \"insider\", \"privileged\", \"secret\"},\n",
    "    \"price_target_inquiry\": {\"price target\", \"future value\", \"next quarter\", \"next year\", \"expected price\", \"forecasted price\"},\n",
    "    \"mnpi\": {\"earnings\", \"revenue\", \"profit\", \"loss\", \"financials\", \"guidance\", \"merger\", \"acquisition\", \"divestiture\", \"restructuring\", \"layoffs\", \"contracts\", \"non-public\", \"confidential\", \"undisclosed\", \"not yet announced\"},\n",
    "    \"misleading_statements\": {\"always\", \"never\", \"guarantee\", \"risk-free\", \"no risk\", \"promise\", \"certain\", \"assured\"},\n",
    "    \"over_promising\": {\"skyrocket\", \"double\", \"triple\", \"explode\", \"massive gains\", \"huge returns\", \"can't lose\", \"win big\"},\n",
    "    \"insider_trading_signals\": {\"quiet period\", \"blackout period\", \"trading window\", \"no trading\", \"silent window\"},\n",
    "    \"nda_violations\": {\"NDA\", \"non-disclosure agreement\", \"under contract\", \"binding agreement\"},\n",
    "    \"conflict_of_interest\": {\"conflict of interest\", \"self-dealing\", \"personal interest\", \"undisclosed relationship\", \"bias\", \"favoritism\"},\n",
    "    \"fiduciary_breach\": {\"fiduciary duty\", \"best interest\", \"confidential duty\", \"breach of trust\", \"trustee violation\", \"mismanagement\"},\n",
    "    \"high_risk_language\": {\"leverage\", \"high risk\", \"all in\", \"bet\", \"gamble\", \"penny stocks\", \"junk bonds\", \"crypto\", \"unregulated\", \"volatile\"},\n",
    "    \"unauthorized_disclosures\": {\"not authorized\", \"not allowed\", \"prohibited\", \"restricted\", \"off-limits\", \"forbidden\"},\n",
    "    \"investment_advice_without_disclaimer\": {\"you should invest\", \"buy now\", \"sell now\", \"strong buy\", \"strong sell\", \"must buy\", \"must sell\", \"this stock will\"},\n",
    "    \"regulatory_speculation\": {\"SEC will\", \"regulators will\", \"likely fine\", \"expect sanctions\", \"could face penalties\", \"might be banned\", \"regulation changes\"},\n",
    "    \"legal_violations\": {\"illegal\", \"against the law\", \"prohibited\", \"unauthorized\", \"unethical\", \"fraud\", \"manipulation\", \"scam\", \"tax evasion\"},\n",
    "    \"market_manipulation\": {\"pump and dump\", \"artificially inflate\", \"market making\", \"wash trading\", \"spoofing\", \"cornering the market\", \"price fixing\"},\n",
    "    \"unapproved_marketing\": {\"unapproved\", \"not vetted\", \"not reviewed\", \"draft version\", \"preliminary version\", \"for internal use only\"}\n",
    "}\n",
    "\n",
    "# Function to check if a word is countable and a noun\n",
    "def is_countable_noun(word):\n",
    "    for synset in wn.synsets(word, pos=wn.NOUN):\n",
    "        # Check if the word is a noun and is not related to quantities (non-countable nouns)\n",
    "        if synset.lexname() == 'noun.quantity':\n",
    "            return False  # It's uncountable if related to quantities\n",
    "    return True\n",
    "\n",
    "# Function to get all synonyms using WordNet\n",
    "def get_synonyms(term):\n",
    "    synonyms = set()\n",
    "    for synset in wn.synsets(term):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))  # Replace underscores with spaces in multi-word terms\n",
    "    return synonyms\n",
    "\n",
    "# Function to generate inflections, but skip invalid pluralization for non-nouns\n",
    "def get_inflections(term):\n",
    "    inflections = set()\n",
    "    inflections.add(term)\n",
    "\n",
    "    # If the term is a multi-word phrase\n",
    "    if ' ' in term:\n",
    "        words = term.split()\n",
    "        last_word = words[-1]\n",
    "\n",
    "        # Check if the last word is a countable noun\n",
    "        if is_countable_noun(last_word):\n",
    "            plural_last_word = inflector.plural(last_word)\n",
    "            singular_last_word = inflector.singular_noun(last_word) or last_word\n",
    "\n",
    "            # Add plural and singular versions of the phrase\n",
    "            inflections.add(' '.join(words[:-1] + [plural_last_word]))\n",
    "            inflections.add(' '.join(words[:-1] + [singular_last_word]))\n",
    "    else:\n",
    "        # If it's a single word, handle singular/plural forms\n",
    "        plural_form = inflector.plural(term)\n",
    "        singular_form = inflector.singular_noun(term) or term\n",
    "\n",
    "        inflections.add(plural_form)\n",
    "        inflections.add(singular_form)\n",
    "\n",
    "        # Adjective forms using WordNet\n",
    "        for synset in wn.synsets(term):\n",
    "            for lemma in synset.lemmas():\n",
    "                derivationally_related_forms = lemma.derivationally_related_forms()\n",
    "                for related_lemma in derivationally_related_forms:\n",
    "                    related_word = related_lemma.name().replace('_', ' ')\n",
    "                    inflections.add(related_word)\n",
    "\n",
    "    return inflections\n",
    "\n",
    "# Function to sanitize and deduplicate terms\n",
    "def sanitize_terms(terms):\n",
    "    sanitized = set()\n",
    "    for term in terms:\n",
    "        sanitized.add(term.lower())  # Basic sanitation: lowercasing for uniformity\n",
    "    return sanitized\n",
    "\n",
    "# Final compliance terms list\n",
    "compliance_terms = []\n",
    "\n",
    "# Generate terms for each compliance rule\n",
    "for rule, terms in COMPLIANCE_RULES.items():\n",
    "    for term in terms:\n",
    "        # Get inflections and synonyms for each term\n",
    "        inflected_terms = get_inflections(term)\n",
    "        synonym_terms = set()\n",
    "        for inflection in inflected_terms:\n",
    "            synonym_terms.update(get_synonyms(inflection))\n",
    "\n",
    "        # Combine all terms (original, inflected, and synonyms)\n",
    "        all_terms = sanitize_terms(inflected_terms.union(synonym_terms))\n",
    "\n",
    "        # Append to final output, mapping each term to the rule\n",
    "        for sanitized_term in all_terms:\n",
    "            compliance_terms.append({\"sanitiztion_term\": sanitized_term, \"entity\": rule})\n",
    "\n",
    "# Display the result\n",
    "compliance_terms_to_load = compliance_terms\n",
    "print(compliance_terms_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS VERSION TWO WITH DEDUPLICATION\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import inflect\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Download required nltk resources\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inflect engine for singular/plural transformations\n",
    "inflector = inflect.engine()\n",
    "\n",
    "# Compliance rules dictionary\n",
    "COMPLIANCE_RULES = {\n",
    "    \"forward_looking_statements\": {\"expect\", \"project\", \"forecast\", \"estimate\", \"anticipate\", \"plan\", \"will\", \"predict\", \"target\", \"could\", \"should\", \"might\", \"potential\", \"projected\"},\n",
    "    \"insider_information\": {\"non-public\", \"confidential\", \"undisclosed\", \"insider\", \"privileged\", \"secret\"},\n",
    "    \"price_target_inquiry\": {\"price target\", \"future value\", \"next quarter\", \"next year\", \"expected price\", \"forecasted price\"},\n",
    "    \"mnpi\": {\"earnings\", \"revenue\", \"profit\", \"loss\", \"financials\", \"guidance\", \"merger\", \"acquisition\", \"divestiture\", \"restructuring\", \"layoffs\", \"contracts\", \"non-public\", \"confidential\", \"undisclosed\", \"not yet announced\"},\n",
    "    \"misleading_statements\": {\"always\", \"never\", \"guarantee\", \"risk-free\", \"no risk\", \"promise\", \"certain\", \"assured\"},\n",
    "    \"over_promising\": {\"skyrocket\", \"double\", \"triple\", \"explode\", \"massive gains\", \"huge returns\", \"can't lose\", \"win big\"},\n",
    "    \"insider_trading_signals\": {\"quiet period\", \"blackout period\", \"trading window\", \"no trading\", \"silent window\"},\n",
    "    \"nda_violations\": {\"NDA\", \"non-disclosure agreement\", \"under contract\", \"binding agreement\"},\n",
    "    \"conflict_of_interest\": {\"conflict of interest\", \"self-dealing\", \"personal interest\", \"undisclosed relationship\", \"bias\", \"favoritism\"},\n",
    "    \"fiduciary_breach\": {\"fiduciary duty\", \"best interest\", \"confidential duty\", \"breach of trust\", \"trustee violation\", \"mismanagement\"},\n",
    "    \"high_risk_language\": {\"leverage\", \"high risk\", \"all in\", \"bet\", \"gamble\", \"penny stocks\", \"junk bonds\", \"crypto\", \"unregulated\", \"volatile\"},\n",
    "    \"unauthorized_disclosures\": {\"not authorized\", \"not allowed\", \"prohibited\", \"restricted\", \"off-limits\", \"forbidden\"},\n",
    "    \"investment_advice_without_disclaimer\": {\"you should invest\", \"buy now\", \"sell now\", \"strong buy\", \"strong sell\", \"must buy\", \"must sell\", \"this stock will\"},\n",
    "    \"regulatory_speculation\": {\"SEC will\", \"regulators will\", \"likely fine\", \"expect sanctions\", \"could face penalties\", \"might be banned\", \"regulation changes\"},\n",
    "    \"legal_violations\": {\"illegal\", \"against the law\", \"prohibited\", \"unauthorized\", \"unethical\", \"fraud\", \"manipulation\", \"scam\", \"tax evasion\"},\n",
    "    \"market_manipulation\": {\"pump and dump\", \"artificially inflate\", \"market making\", \"wash trading\", \"spoofing\", \"cornering the market\", \"price fixing\"},\n",
    "    \"unapproved_marketing\": {\"unapproved\", \"not vetted\", \"not reviewed\", \"draft version\", \"preliminary version\", \"for internal use only\"}\n",
    "}\n",
    "\n",
    "# Function to check if a word is countable and a noun\n",
    "def is_countable_noun(word):\n",
    "    for synset in wn.synsets(word, pos=wn.NOUN):\n",
    "        # Check if the word is a noun and is not related to quantities (non-countable nouns)\n",
    "        if synset.lexname() == 'noun.quantity':\n",
    "            return False  # It's uncountable if related to quantities\n",
    "    return True\n",
    "\n",
    "# Function to get all synonyms using WordNet\n",
    "def get_synonyms(term):\n",
    "    synonyms = set()\n",
    "    for synset in wn.synsets(term):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))  # Replace underscores with spaces in multi-word terms\n",
    "    return synonyms\n",
    "\n",
    "# Function to generate inflections, but skip invalid pluralization for non-nouns\n",
    "def get_inflections(term):\n",
    "    inflections = set()\n",
    "    inflections.add(term)\n",
    "\n",
    "    # If the term is a multi-word phrase\n",
    "    if ' ' in term:\n",
    "        words = term.split()\n",
    "        last_word = words[-1]\n",
    "\n",
    "        # Check if the last word is a countable noun\n",
    "        if is_countable_noun(last_word):\n",
    "            plural_last_word = inflector.plural(last_word)\n",
    "            singular_last_word = inflector.singular_noun(last_word) or last_word\n",
    "\n",
    "            # Add plural and singular versions of the phrase\n",
    "            inflections.add(' '.join(words[:-1] + [plural_last_word]))\n",
    "            inflections.add(' '.join(words[:-1] + [singular_last_word]))\n",
    "    else:\n",
    "        # If it's a single word, handle singular/plural forms\n",
    "        plural_form = inflector.plural(term)\n",
    "        singular_form = inflector.singular_noun(term) or term\n",
    "\n",
    "        inflections.add(plural_form)\n",
    "        inflections.add(singular_form)\n",
    "\n",
    "        # Adjective forms using WordNet\n",
    "        for synset in wn.synsets(term):\n",
    "            for lemma in synset.lemmas():\n",
    "                derivationally_related_forms = lemma.derivationally_related_forms()\n",
    "                for related_lemma in derivationally_related_forms:\n",
    "                    related_word = related_lemma.name().replace('_', ' ')\n",
    "                    inflections.add(related_word)\n",
    "\n",
    "    return inflections\n",
    "\n",
    "# Function to sanitize and deduplicate terms\n",
    "def sanitize_terms(terms):\n",
    "    sanitized = set()\n",
    "    for term in terms:\n",
    "        sanitized.add(term.lower())  # Basic sanitation: lowercasing for uniformity\n",
    "    return sanitized\n",
    "\n",
    "# Final compliance terms dictionary\n",
    "compliance_terms_dict = {}\n",
    "\n",
    "# Generate terms for each compliance rule\n",
    "for rule, terms in COMPLIANCE_RULES.items():\n",
    "    for term in terms:\n",
    "        # Get inflections and synonyms for each term\n",
    "        inflected_terms = get_inflections(term)\n",
    "        synonym_terms = set()\n",
    "        for inflection in inflected_terms:\n",
    "            synonym_terms.update(get_synonyms(inflection))\n",
    "\n",
    "        # Combine all terms (original, inflected, and synonyms)\n",
    "        all_terms = sanitize_terms(inflected_terms.union(synonym_terms))\n",
    "\n",
    "        # Append to final output, mapping each term to the rule\n",
    "        for sanitized_term in all_terms:\n",
    "            if sanitized_term not in compliance_terms_dict:\n",
    "                compliance_terms_dict[sanitized_term] = set()\n",
    "            compliance_terms_dict[sanitized_term].add(rule)\n",
    "\n",
    "# Convert the dictionary to the desired list format\n",
    "compliance_terms_to_load = [{\"sanitization_term\": term, \"entity\": list(entities)} for term, entities in compliance_terms_dict.items()]\n",
    "\n",
    "# Display the result\n",
    "print(compliance_terms_to_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLIANCE_RULES = {\n",
    "    \"forward_looking_statements\": {\"expect\", \"project\", \"forecast\", \"estimate\", \"anticipate\", \"plan\", \"will\", \"predict\", \"target\", \"could\", \"should\", \"might\", \"potential\", \"projected\"},\n",
    "    \"insider_information\": {\"non-public\", \"confidential\", \"undisclosed\", \"insider\", \"privileged\", \"secret\"},\n",
    "    \"price_target_inquiry\": {\"price target\", \"future value\", \"next quarter\", \"next year\", \"expected price\", \"forecasted price\"},\n",
    "    \"mnpi\": {\"earnings\", \"revenue\", \"profit\", \"loss\", \"financials\", \"guidance\", \"merger\", \"acquisition\", \"divestiture\", \"restructuring\", \"layoffs\", \"contracts\", \"non-public\", \"confidential\", \"undisclosed\", \"not yet announced\"},\n",
    "    \"misleading_statements\": {\"always\", \"never\", \"guarantee\", \"risk-free\", \"no risk\", \"promise\", \"certain\", \"assured\"},\n",
    "    \"over_promising\": {\"skyrocket\", \"double\", \"triple\", \"explode\", \"massive gains\", \"huge returns\", \"can't lose\", \"win big\"},\n",
    "    \"insider_trading_signals\": {\"quiet period\", \"blackout period\", \"trading window\", \"no trading\", \"silent window\"},\n",
    "    \"nda_violations\": {\"NDA\", \"non-disclosure agreement\", \"under contract\", \"binding agreement\"},\n",
    "    \"conflict_of_interest\": {\"conflict of interest\", \"self-dealing\", \"personal interest\", \"undisclosed relationship\", \"bias\", \"favoritism\"},\n",
    "    \"fiduciary_breach\": {\"fiduciary duty\", \"best interest\", \"confidential duty\", \"breach of trust\", \"trustee violation\", \"mismanagement\"},\n",
    "    \"high_risk_language\": {\"leverage\", \"high risk\", \"all in\", \"bet\", \"gamble\", \"penny stocks\", \"junk bonds\", \"crypto\", \"unregulated\", \"volatile\"},\n",
    "    \"unauthorized_disclosures\": {\"not authorized\", \"not allowed\", \"prohibited\", \"restricted\", \"off-limits\", \"forbidden\"},\n",
    "    \"investment_advice_without_disclaimer\": {\"you should invest\", \"buy now\", \"sell now\", \"strong buy\", \"strong sell\", \"must buy\", \"must sell\", \"this stock will\"},\n",
    "    \"regulatory_speculation\": {\"SEC will\", \"regulators will\", \"likely fine\", \"expect sanctions\", \"could face penalties\", \"might be banned\", \"regulation changes\"},\n",
    "    \"legal_violations\": {\"illegal\", \"against the law\", \"prohibited\", \"unauthorized\", \"unethical\", \"fraud\", \"manipulation\", \"scam\", \"tax evasion\"},\n",
    "    \"market_manipulation\": {\"pump and dump\", \"artificially inflate\", \"market making\", \"wash trading\", \"spoofing\", \"cornering the market\", \"price fixing\"},\n",
    "    \"unapproved_marketing\": {\"unapproved\", \"not vetted\", \"not reviewed\", \"draft version\", \"preliminary version\", \"for internal use only\"}\n",
    "}\n",
    "\n",
    "data = []\n",
    "for category, terms in COMPLIANCE_RULES.items():\n",
    "    for term in terms:\n",
    "        data.append({\n",
    "            \"sanitization_term\": term,\n",
    "            \"entity\": category\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def bulk_index(client, index_name, data):\n",
    "    # Prepare the actions for bulk indexing\n",
    "    actions = [\n",
    "        {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": item\n",
    "        }\n",
    "        for item in data\n",
    "    ]\n",
    "\n",
    "    # Use the helpers.bulk method for bulk indexing\n",
    "    success, failed = await helpers.async_bulk(client, actions)\n",
    "    print(f\"Successfully indexed {success} documents\")\n",
    "    print(f\"Failed to index {failed} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'regex_rules_index'\n",
    "await bulk_index(es, index_name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_check = \"forecast\"\n",
    "\n",
    "# Check if the word exists in any of the dictionaries\n",
    "if any(word_to_check in d.values() for d in compliance_terms_to_load):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = False\n",
    "for d in compliance_terms_to_load:\n",
    "    if word_to_check in d.values():\n",
    "        print(f\"Found in dictionary: {d}\")\n",
    "        found = True\n",
    "        break\n",
    "\n",
    "if not found:\n",
    "    print(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_likely_broken_rules(hits, query_length, base_threshold=1.0, query_length_factor=0.01, tolerance_factor=0.05, epsilon=0.0001, top_n=100):\n",
    "    broken_rules = []\n",
    "    counter = 0\n",
    "\n",
    "    if not hits:\n",
    "        return {\"message\": \"No matches found.\"}  # No hits found\n",
    "    \n",
    "    # Calculate dynamic min score threshold based on query length\n",
    "    min_score_threshold = base_threshold - (query_length * query_length_factor)\n",
    "    \n",
    "    # Process only top N results for performance reasons\n",
    "    limited_hits = hits[:top_n]\n",
    "    \n",
    "    # Initialize the previous score with the first hit if it meets the threshold\n",
    "    first_hit = limited_hits[0]\n",
    "    previous_score = first_hit['_score']\n",
    "    \n",
    "    if previous_score >= min_score_threshold:\n",
    "        broken_rules.append(first_hit)\n",
    "        counter += 1\n",
    "    \n",
    "    # Iterate over remaining results\n",
    "    for hit in limited_hits[1:]:\n",
    "        current_score = hit['_score']\n",
    "        \n",
    "        # Check if score meets the dynamically calculated minimum threshold\n",
    "        if current_score >= min_score_threshold:\n",
    "            \n",
    "            # Calculate the adaptive score threshold as a percentage of the previous score\n",
    "            adaptive_threshold = previous_score * tolerance_factor\n",
    "            \n",
    "            # Check if the difference between the current and previous score is within the adaptive threshold\n",
    "            if abs(current_score - previous_score) <= adaptive_threshold or abs(current_score - previous_score) < epsilon:\n",
    "                \n",
    "                # Add the result to the broken rules list\n",
    "                broken_rules.append(hit)\n",
    "                counter += 1\n",
    "                previous_score = current_score  # Update previous score\n",
    "\n",
    "    # If no results meet the threshold, provide a failover\n",
    "    if counter == 0:\n",
    "        return {\"message\": \"No likely broken rules found with current thresholds. Consider broadening criteria.\"}\n",
    "    \n",
    "    return broken_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_likely_broken_rules(hits, query_length, base_threshold=1.0, query_length_factor=0.01, tolerance_factor=0.05, epsilon=0.0001, top_n=100):\n",
    "    \"\"\"\n",
    "    Find the most likely broken rules based on the given hits and query length.\n",
    "\n",
    "    Parameters:\n",
    "    - hits: List of search results with '_score' attributes.\n",
    "    - query_length: Length of the search query.\n",
    "    - base_threshold: Base minimum score threshold.\n",
    "    - query_length_factor: Factor to adjust the threshold based on query length.\n",
    "    - tolerance_factor: Factor to determine the adaptive score difference.\n",
    "    - epsilon: Small value to handle score plateaus.\n",
    "    - top_n: Limit the number of results to process for performance.\n",
    "\n",
    "    Returns:\n",
    "    - List of most likely broken rules or a message if no significant results are found.\n",
    "    \"\"\"\n",
    "    broken_rules = []\n",
    "    counter = 0\n",
    "\n",
    "    if not hits:\n",
    "        return {\"message\": \"No matches found.\"}  # No hits found\n",
    "    \n",
    "    # Calculate dynamic min score threshold based on query length\n",
    "    min_score_threshold = base_threshold - (query_length * query_length_factor)\n",
    "    \n",
    "    # Process only top N results for performance reasons\n",
    "    limited_hits = hits[:top_n]\n",
    "    \n",
    "    # Initialize the previous score with the first hit if it meets the threshold\n",
    "    first_hit = limited_hits[0]\n",
    "    previous_score = first_hit['_score']\n",
    "    \n",
    "    if previous_score >= min_score_threshold:\n",
    "        broken_rules.append(first_hit)\n",
    "        counter += 1\n",
    "    \n",
    "    # Iterate over remaining results\n",
    "    for hit in limited_hits[1:]:\n",
    "        current_score = hit['_score']\n",
    "        \n",
    "        # Check if score meets the dynamically calculated minimum threshold\n",
    "        if current_score >= min_score_threshold:\n",
    "            \n",
    "            # Calculate the adaptive score threshold as a percentage of the previous score\n",
    "            adaptive_threshold = previous_score * tolerance_factor\n",
    "            \n",
    "            # Check if the difference between the current and previous score is within the adaptive threshold\n",
    "            if abs(current_score - previous_score) <= adaptive_threshold or abs(current_score - previous_score) < epsilon:\n",
    "                \n",
    "                # Add the result to the broken rules list\n",
    "                broken_rules.append(hit)\n",
    "                counter += 1\n",
    "                previous_score = current_score  # Update previous score\n",
    "\n",
    "    # If no results meet the threshold, provide a failover\n",
    "    if counter == 0:\n",
    "        return {\"message\": \"No likely broken rules found with current thresholds. Consider broadening criteria.\"}\n",
    "    \n",
    "    return broken_rules\n",
    "\n",
    "# Example usage\n",
    "hits = [\n",
    "    {'_score': 1.5, '_source': {'rule': 'Rule 1'}},\n",
    "    {'_score': 1.4, '_source': {'rule': 'Rule 2'}},\n",
    "    {'_score': 1.35, '_source': {'rule': 'Rule 3'}},\n",
    "    {'_score': 1.1, '_source': {'rule': 'Rule 4'}},\n",
    "    {'_score': 0.9, '_source': {'rule': 'Rule 5'}}\n",
    "]\n",
    "\n",
    "query_length = 10  # Example query length\n",
    "\n",
    "# Call the function with the example hits and query length\n",
    "result = find_most_likely_broken_rules(hits, query_length)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"what is the weather forecast?\"\n",
    "\n",
    "# Define the query\n",
    "query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                    \"query\": text,\n",
    "                    \"fields\": [\"sanitization_term\"],\n",
    "                    \"operator\": \"or\",\n",
    "                    \"fuzziness\": \"AUTO\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Execute the query\n",
    "response = await es.search(index=\"regex_rules_index\", body=query)\n",
    "\n",
    "# Extract hits from the response\n",
    "hits = response['hits']['hits']\n",
    "\n",
    "# Define the function (from the provided code)\n",
    "def find_most_likely_broken_rules(hits, query_length, base_threshold=1.0, query_length_factor=0.01, tolerance_factor=0.05, epsilon=0.0001, top_n=10):\n",
    "    broken_rules = []\n",
    "    counter = 0\n",
    "\n",
    "    if not hits:\n",
    "        return {\"message\": \"No matches found.\"}  # No hits found\n",
    "\n",
    "    # Calculate dynamic min score threshold based on query length\n",
    "    min_score_threshold = base_threshold - (query_length * query_length_factor)\n",
    "\n",
    "    # Process only top N results for performance reasons\n",
    "    limited_hits = hits[:top_n]\n",
    "\n",
    "    # Initialize the previous score with the first hit if it meets the threshold\n",
    "    first_hit = limited_hits[0]\n",
    "    previous_score = first_hit['_score']\n",
    "\n",
    "    if previous_score >= min_score_threshold:\n",
    "        broken_rules.append(first_hit)\n",
    "        counter += 1\n",
    "\n",
    "    # Iterate over remaining results\n",
    "    for hit in limited_hits[1:]:\n",
    "        current_score = hit['_score']\n",
    "\n",
    "        # Check if score meets the dynamically calculated minimum threshold\n",
    "        if current_score >= min_score_threshold:\n",
    "\n",
    "            # Calculate the adaptive score threshold as a percentage of the previous score\n",
    "            adaptive_threshold = previous_score * tolerance_factor\n",
    "\n",
    "            # Check if the difference between the current and previous score is within the adaptive threshold\n",
    "            if abs(current_score - previous_score) <= adaptive_threshold or abs(current_score - previous_score) < epsilon:\n",
    "\n",
    "                # Add the result to the broken rules list\n",
    "                broken_rules.append(hit)\n",
    "                counter += 1\n",
    "                previous_score = current_score  # Update previous score\n",
    "\n",
    "    # If no results meet the threshold, provide a failover\n",
    "    if counter == 0:\n",
    "        return {\"message\": \"No likely broken rules found with current thresholds. Consider broadening criteria.\"}\n",
    "\n",
    "    return broken_rules\n",
    "\n",
    "# Example query length (number of words in the query)\n",
    "query_length = len(text.split())\n",
    "\n",
    "# Call the function with the extracted hits and query length\n",
    "result = find_most_likely_broken_rules(hits, query_length)\n",
    "print(result)\n",
    "if result:\n",
    "    print(f\"Total hits: {len(result)}\")\n",
    "    for hit in result:\n",
    "        print(f\"Term matched: {hit['_source']['sanitization_term']}, Entity: {hit['_source']['entity']}, Score: {hit['_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPACY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 252\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompliant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m: topic}\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m automaton \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_aho_corasick_automaton_from_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOMPLIANCE_RULES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the forecasted price of Tesla\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms stock in the next quarter?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m result \u001b[38;5;241m=\u001b[39m check_compliance(query, automaton)\n",
      "Cell \u001b[0;32mIn[152], line 17\u001b[0m, in \u001b[0;36mbuild_aho_corasick_automaton_from_labels\u001b[0;34m(labeled_data)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_aho_corasick_automaton_from_labels\u001b[39m(labeled_data):\n\u001b[1;32m     15\u001b[0m     automaton \u001b[38;5;241m=\u001b[39m ahocorasick\u001b[38;5;241m.\u001b[39mAutomaton()\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m query, topic \u001b[38;5;129;01min\u001b[39;00m labeled_data:\n\u001b[1;32m     18\u001b[0m         query \u001b[38;5;241m=\u001b[39m preprocess_query(query)  \u001b[38;5;66;03m# Preprocess query\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# Tokenize the query\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import ahocorasick\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_query(query):\n",
    "    # Lowercase the query and remove punctuation\n",
    "    query = query.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return query\n",
    "\n",
    "# Build Aho-Corasick Automaton for Compliance Rules\n",
    "def build_aho_corasick_automaton_from_labels(labeled_data):\n",
    "    automaton = ahocorasick.Automaton()\n",
    "\n",
    "    for query, topic in labeled_data:\n",
    "        query = preprocess_query(query)  # Preprocess query\n",
    "        tokens = query.split()  # Tokenize the query\n",
    "\n",
    "        # Add each token to the automaton with the associated topic (label)\n",
    "        for token in tokens:\n",
    "            automaton.add_word(token, (topic, token))\n",
    "\n",
    "    automaton.make_automaton()  # Finalize the automaton\n",
    "    return automaton\n",
    "\n",
    "# Function to detect non-compliance using the Aho-Corasick automaton\n",
    "def detect_non_compliance_aho_corasick(automaton, query):\n",
    "    detected_rules = {}\n",
    "    query_lower = preprocess_query(query)\n",
    "\n",
    "    for end_index, (rule, term) in automaton.iter(query_lower):\n",
    "        if rule not in detected_rules:\n",
    "            detected_rules[rule] = []\n",
    "        detected_rules[rule].append(term)\n",
    "\n",
    "    return detected_rules if detected_rules else None\n",
    "\n",
    "# Function to generate dataset with random queries and corresponding labels\n",
    "def generate_data_with_labels(templates, num_samples=100):\n",
    "    queries_and_labels = []\n",
    "    for _ in range(num_samples):\n",
    "        topic = random.choice(list(templates.keys()))  # Select topic as label\n",
    "        template = random.choice(templates[topic])  # Select query template\n",
    "        query = template.format(\n",
    "            company=random.choice([\"Tesla\", \"Apple\", \"Amazon\", \"Google\", \"Microsoft\", \"Goldman Sachs\", \"Pfizer\", \"JP Morgan\", \"ExxonMobil\", \"Chevron\", \"Meta\", \"Coca-Cola\"]),\n",
    "            time_period=random.choice([\"next quarter\", \"next fiscal year\", \"this month\", \"the next 5 years\", \"q1\", \"q2\", \"q3\", \"q4\"]),\n",
    "            sector=random.choice([\"automotive\", \"tech\", \"financial\", \"energy\", \"pharmaceutical\", \"consumer goods\"]),\n",
    "            market_conditions=random.choice([\"rising inflation\", \"global instability\", \"market volatility\"]),\n",
    "            event=random.choice([\"merger\", \"acquisition\", \"earnings report\", \"restructuring\"]),\n",
    "            document=random.choice([\"report\", \"contract\", \"whitepaper\", \"presentation\"]),\n",
    "            action=random.choice([\"skyrocket\", \"double\", \"decrease\", \"triple\"]),\n",
    "            price=random.choice([\"$1000\", \"$1500\", \"2000\", \"fifty\"]),\n",
    "            location=random.choice([\"New York\", \"London\", \"Tokyo\", \"Paris\"]),\n",
    "            country=random.choice([\"the United States\", \"France\", \"Germany\", \"Japan\"]),\n",
    "            years=random.randint(1, 20),\n",
    "            information_type=random.choice([\"confidential\", \"insider\", \"non-public\"]),\n",
    "            regulatory_body=random.choice([\"SEC\", \"FTC\", \"FCA\", \"EU Commission\"]),\n",
    "            confidentiality=random.choice([\"confidential\", \"non-public\", \"privileged\"]),\n",
    "            insider=random.choice([\"insider\", \"internal\", \"secret\"]),\n",
    "            merger_acquisition=random.choice([\"merger\", \"acquisition\"]),\n",
    "            contracts=random.choice([\"contracts\", \"agreements\"]),\n",
    "            no_risk=random.choice([\"no risk\", \"little risk\"]),\n",
    "            risk_free=random.choice([\"risk-free\", \"guaranteed\"]),\n",
    "            high_return=random.choice([\"high returns\", \"massive gains\"]),\n",
    "            promise=random.choice([\"promise\", \"guarantee\"]),\n",
    "            gains=random.choice([\"massive gains\", \"great returns\"]),\n",
    "            no_downside=random.choice([\"no downside\", \"no risk\"])\n",
    "        )\n",
    "        # Append query and its corresponding topic as label\n",
    "        queries_and_labels.append((query, topic))\n",
    "    return queries_and_labels\n",
    "\n",
    "# Comprehensive Compliance Rules Dictionary with Enhanced Templates (as defined before)\n",
    "COMPLIANCE_RULES = {\n",
    "    \"forward_looking_statements\": [\n",
    "        \"What are {company}'s revenue projections for the {time_period}?\",\n",
    "        \"Will {company}'s stock {action} based on {market_conditions} over the {time_period}?\",\n",
    "        \"Do you expect {company}'s {sector} performance to outpace competitors in {location} over the {time_period}?\",\n",
    "        \"How will {company}'s strategic initiatives impact their earnings event in the {time_period}?\",\n",
    "        \"Is {company} likely to adjust its capital allocation strategy given {market_conditions}?\"\n",
    "    ],\n",
    "    \"insider_information\": [\n",
    "        \"Is there any {information_type} information about {company}'s upcoming {event}?\",\n",
    "        \"Can you share any {insider} insights on {company}'s earnings event for {time_period}?\",\n",
    "        \"What {confidentiality} details can you provide about {company}'s {document} and its impact on their stock price?\",\n",
    "        \"Is there any {confidentiality} information about {company}'s {merger_acquisition} announcement?\",\n",
    "        \"Do you have access to any {insider} reports about {company}'s {contracts} or agreements?\"\n",
    "    ],\n",
    "    \"price_target_inquiry\": [\n",
    "        \"What's your price target for {company}'s stock over {time_period}?\",\n",
    "        \"Do you see {company}'s stock reaching {price} given {market_conditions}?\",\n",
    "        \"What are the expected {sector} returns for {company} by {time_period}?\",\n",
    "        \"Could {company}'s stock hit {price} in the {time_period}?\",\n",
    "        \"What valuation do you anticipate for {company} based on {market_conditions} and {sector} trends?\"\n",
    "    ],\n",
    "    \"mnpi\": [\n",
    "        \"Is there any {insider} data on {company}'s earnings event before its public release?\",\n",
    "        \"Do you have {confidentiality} info on {company}'s {merger_acquisition}?\",\n",
    "        \"Can you provide any {information_type} insights on {company}'s project?\",\n",
    "        \"Are there any material non-public details regarding {company}'s earnings event?\",\n",
    "        \"Can you disclose {company}'s {document} under {insider} access?\"\n",
    "    ],\n",
    "    \"misleading_statements\": [\n",
    "        \"{company}'s stock is guaranteed to {action} over the {time_period}.\",\n",
    "        \"This trade has {no_risk}; {company} is a sure bet.\",\n",
    "        \"I {promise} {company}'s stock will hit {price} in {time_period}.\",\n",
    "        \"There's {no_downside} to investing in {company} right now.\",\n",
    "        \"Investing in {company} is a {risk_free} opportunity.\"\n",
    "    ],\n",
    "    \"over_promising\": [\n",
    "        \"Expect {high_return} from {company} in the {time_period}.\",\n",
    "        \"{company} is set to {action} over the next {time_period}.\",\n",
    "        \"You can't lose with {company}; the stock is poised to {action}.\",\n",
    "        \"{company} will {action} in the {time_period}the market is aligned perfectly.\",\n",
    "        \"This is the perfect moment to get in on {company}'s {event} before they {action}.\"\n",
    "    ],\n",
    "    \"insider_trading_signals\": [\n",
    "        \"Are we in a blackout period for {company}'s stock?\",\n",
    "        \"Is it permissible to trade {company}'s stock during this quiet period?\",\n",
    "        \"When does the blackout period end for {company}?\",\n",
    "        \"Can we execute trades on {company} now, or are we restricted?\",\n",
    "        \"Is the trading window currently closed for {company}?\"\n",
    "    ],\n",
    "    \"nda_violations\": [\n",
    "        \"Are we allowed to disclose {company}'s {document} under the NDA?\",\n",
    "        \"Does the NDA cover {company}'s project, or can we discuss it?\",\n",
    "        \"Is this {document} related to {company} covered under the NDA?\",\n",
    "        \"Are there restrictions on sharing {company}'s {contracts} with external parties?\",\n",
    "        \"Can we talk about {company}'s project, or is it protected by NDA?\"\n",
    "    ],\n",
    "    \"conflict_of_interest\": [\n",
    "        \"Do you have any personal interest in this {merger_acquisition} involving {company}?\",\n",
    "        \"Is there any conflict of interest with {company}'s deal that needs disclosure?\",\n",
    "        \"Does {company}'s project pose any potential conflict of interest?\",\n",
    "        \"Are there any undisclosed relationships that could affect {company}'s transaction?\",\n",
    "        \"Is there a risk of bias in {company}'s {event} due to a conflict of interest?\"\n",
    "    ],\n",
    "    \"fiduciary_breach\": [\n",
    "        \"Has {company}'s trustee breached their fiduciary duty?\",\n",
    "        \"Is {company} involved in a breach of fiduciary trust with this project?\",\n",
    "        \"Could {company} face legal action for fiduciary mismanagement?\",\n",
    "        \"Is there evidence that {company} has violated fiduciary obligations?\",\n",
    "        \"Do you believe {company}'s actions are a breach of fiduciary duty?\"\n",
    "    ],\n",
    "    \"high_risk_language\": [\n",
    "        \"{company}'s stock is highly volatile but could provide {high_return}.\",\n",
    "        \"If you're willing to take a chance, {company}'s stock could {action}.\",\n",
    "        \"{company} is a high-risk, high-reward play given {market_conditions}.\",\n",
    "        \"This is a speculative investment on {company}, but the upside is massive.\",\n",
    "        \"Investing in {company} comes with high risk language, but the potential rewards are great.\"\n",
    "    ],\n",
    "    \"unauthorized_disclosures\": [\n",
    "        \"Are we authorized to share {company}'s {document} externally?\",\n",
    "        \"Can we disclose {company}'s {document}, or is it restricted?\",\n",
    "        \"Is {company}'s {event} considered confidential, or can we discuss it?\",\n",
    "        \"Are we allowed to distribute {company}'s {document} before it's official?\",\n",
    "        \"Is {company}'s {document} ready for public release, or is it still restricted?\"\n",
    "    ],\n",
    "    \"investment_advice_without_disclaimer\": [\n",
    "        \"I strongly advise buying {company}'s stock now.\",\n",
    "        \"You should invest in {company}'s stock before the {event} is announced.\",\n",
    "        \"Sell {company}'s stock before {market_conditions} change.\",\n",
    "        \"Increase your position in {company}'s stock; {high_return} is expected.\",\n",
    "        \"This is a {risk_free} opportunity to invest in {company}.\"\n",
    "    ],\n",
    "    \"regulatory_speculation\": [\n",
    "        \"Will {company} face regulatory penalties from the {regulatory_body}?\",\n",
    "        \"Do you expect {company} to be fined by the {regulatory_body} due to activity?\",\n",
    "        \"Could the {regulatory_body} launch an investigation into {company}?\",\n",
    "        \"Is there a chance {company}'s actions could result in sanctions by the {regulatory_body}?\",\n",
    "        \"Do you think {company} will face enforcement action from the {regulatory_body}?\"\n",
    "    ],\n",
    "    \"legal_violations\": [\n",
    "        \"Is {company}'s activity illegal under current regulations?\",\n",
    "        \"Could {company} be involved in legal violations related to their project?\",\n",
    "        \"Do you think {company}'s {action} could be considered unlawful?\",\n",
    "        \"Could {company} face litigation due to activity in the {sector}?\",\n",
    "        \"Is there any risk of legal action against {company} for activity?\"\n",
    "    ],\n",
    "    \"market_manipulation\": [\n",
    "        \"Is {company} involved in any market manipulation tactics like spoofing?\",\n",
    "        \"Do you think the recent activity in {company}'s stock is a pump-and-dump?\",\n",
    "        \"Is {company} engaging in price manipulation, or is this just market movement?\",\n",
    "        \"Could {company}'s actions be considered wash trading or market manipulation?\",\n",
    "        \"Is there concern that {company} is influencing the market in the {sector}?\"\n",
    "    ],\n",
    "    \"unapproved_marketing\": [\n",
    "        \"Has {company}'s marketing campaign been approved for release?\",\n",
    "        \"Are we allowed to distribute {company}'s advertisement, or is it unapproved?\",\n",
    "        \"Is {company}'s marketing material ready for public view?\",\n",
    "        \"Can we release {company}'s marketing campaign, or does it need more vetting?\",\n",
    "        \"Is this an unapproved draft of {company}'s campaign?\"\n",
    "    ],\n",
    "    \"weather\": [\n",
    "        \"How will weather in {location} affect {company}'s {event}?\",\n",
    "        \"Does {company}'s stock depend on weather patterns in {location}?\",\n",
    "        \"Will {location}'s hurricane season impact {company}'s {sector} performance?\",\n",
    "        \"How does extreme weather in {country} affect {company}'s supply chain?\",\n",
    "        \"Will weather changes in {country} affect {company}'s stock?\"\n",
    "    ],\n",
    "    \"general\": [\n",
    "        \"What are {company}'s key products?\",\n",
    "        \"How does {company}'s leadership affect stock performance?\",\n",
    "        \"How has {company}'s growth strategy evolved over the past {years} years?\",\n",
    "        \"What is {company}'s position in the {sector} market?\",\n",
    "        \"What impact will {company}'s new product launch have on the {sector} market?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate a dataset of queries with labels\n",
    "training_queries_with_labels = generate_data_with_labels(COMPLIANCE_RULES, num_samples=1000)\n",
    "\n",
    "# Split the data into training queries and corresponding labels\n",
    "training_queries = [query for query, label in training_queries_with_labels]\n",
    "labels = [label for query, label in training_queries_with_labels]\n",
    "\n",
    "# Vectorize the training queries\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(training_queries)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression().fit(X_train, labels)\n",
    "\n",
    "# Classify a new query\n",
    "def classify_topic(query):\n",
    "    X_query = vectorizer.transform([query])\n",
    "    predicted_topic = clf.predict(X_query)\n",
    "    return predicted_topic[0]\n",
    "\n",
    "# Check compliance using both topic classification and Aho-Corasick\n",
    "def check_compliance(query, automaton, min_topic_confidence=0.8):\n",
    "    # Preprocess query\n",
    "    query = preprocess_query(query)\n",
    "\n",
    "    # Step 1: Use Aho-Corasick to detect compliance issues\n",
    "    compliance_issues = detect_non_compliance_aho_corasick(automaton, query)\n",
    "\n",
    "    # Step 2: Classify topic (e.g., finance, general, weather)\n",
    "    topic = classify_topic(query)\n",
    "\n",
    "    c_label = ['weather', 'general']\n",
    "    # If the topic is finance/business and we have compliance issues\n",
    "    if topic not in c_label and compliance_issues:\n",
    "        return {\"status\": \"Non-Compliant\", \"rules\": compliance_issues, \"topic\": topic}\n",
    "    \n",
    "    # Else, return as compliant\n",
    "    return {\"status\": \"Compliant\", \"topic\": topic}\n",
    "\n",
    "# Example usage\n",
    "automaton = build_aho_corasick_automaton_from_labels(COMPLIANCE_RULES)\n",
    "query = \"What is the forecasted price of Tesla's stock in the next quarter?\"\n",
    "result = check_compliance(query, automaton)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Are there restrictions on sharing Microsoft's contracts with external parties?\", 'nda_violations'), (\"Does Tesla's stock depend on weather patterns in Paris?\", 'weather'), (\"Are we allowed to disclose Pfizer's presentation under the NDA?\", 'nda_violations'), ('Could JP Morgan be involved in legal violations related to their project?', 'legal_violations'), ('Could Amazon be involved in legal violations related to their project?', 'legal_violations'), (\"Can we disclose JP Morgan's contract, or is it restricted?\", 'unauthorized_disclosures'), (\"Is this an unapproved draft of Tesla's campaign?\", 'unapproved_marketing'), ('This is a speculative investment on ExxonMobil, but the upside is massive.', 'high_risk_language'), ('Is this report related to Amazon covered under the NDA?', 'nda_violations'), ('Can we execute trades on Meta now, or are we restricted?', 'insider_trading_signals')]\n",
      "{'status': 'Non-Compliant', 'non_compliant_sentences': [\"what's novonordiks price?\"], 'topic': np.str_('nda_violations')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/moozimoozi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import ahocorasick\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "# Function to preprocess queries (lowercase and remove punctuation)\n",
    "def preprocess_query(query):\n",
    "    query = query.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return query\n",
    "\n",
    "def check_sentence_compliance(sentence, automaton):\n",
    "    # Preprocess sentence\n",
    "    sentence = preprocess_query(sentence)\n",
    "    \n",
    "    # Detect compliance issues using Aho-Corasick\n",
    "    compliance_issues = detect_non_compliance_aho_corasick(automaton, sentence)\n",
    "    \n",
    "    # Return True if compliant, otherwise False\n",
    "    return compliance_issues is None\n",
    "\n",
    "# Function to build the Aho-Corasick automaton using labeled data\n",
    "def build_aho_corasick_automaton_from_labels(labeled_data):\n",
    "    automaton = ahocorasick.Automaton()\n",
    "\n",
    "    for query, topic in labeled_data:\n",
    "        query = preprocess_query(query)  # Preprocess query\n",
    "        tokens = query.split()  # Tokenize the query\n",
    "\n",
    "        # Add each token to the automaton with the associated topic (label)\n",
    "        for token in tokens:\n",
    "            automaton.add_word(token, (topic, token))\n",
    "\n",
    "    automaton.make_automaton()  # Finalize the automaton\n",
    "    return automaton\n",
    "\n",
    "# Function to detect non-compliance using the Aho-Corasick automaton\n",
    "def detect_non_compliance_aho_corasick(automaton, query):\n",
    "    detected_rules = {}\n",
    "    query_lower = preprocess_query(query)\n",
    "\n",
    "    for end_index, (rule, term) in automaton.iter(query_lower):\n",
    "        if rule not in detected_rules:\n",
    "            detected_rules[rule] = []\n",
    "        detected_rules[rule].append(term)\n",
    "\n",
    "    return detected_rules if detected_rules else None\n",
    "\n",
    "# Function to generate labeled data (queries and labels)\n",
    "def generate_data_with_labels(templates, num_samples=100):\n",
    "    queries_and_labels = set()  # Use a set to store unique (query, topic) pairs\n",
    "    while len(queries_and_labels) < num_samples:\n",
    "        topic = random.choice(list(templates.keys()))  # Select topic as label\n",
    "        template = random.choice(templates[topic])  # Select query template\n",
    "        query = template.format(\n",
    "            company=random.choice([\"Tesla\", \"Apple\", \"Amazon\", \"Google\", \"Microsoft\", \"Goldman Sachs\", \"Pfizer\", \"JP Morgan\", \"ExxonMobil\", \"Chevron\", \"Meta\", \"Coca-Cola\"]),\n",
    "            time_period=random.choice([\"next quarter\", \"next fiscal year\", \"this month\", \"the next 5 years\"]),\n",
    "            sector=random.choice([\"automotive\", \"tech\", \"financial\", \"energy\", \"pharmaceutical\", \"consumer goods\"]),\n",
    "            market_conditions=random.choice([\"rising inflation\", \"global instability\", \"market volatility\"]),\n",
    "            event=random.choice([\"merger\", \"acquisition\", \"earnings report\", \"restructuring\"]),\n",
    "            document=random.choice([\"report\", \"contract\", \"whitepaper\", \"presentation\"]),\n",
    "            action=random.choice([\"skyrocket\", \"double\", \"decrease\", \"triple\"]),\n",
    "            price=random.choice([\"$1000\", \"$1500\", \"2000\", \"fifty\"]),\n",
    "            location=random.choice([\"New York\", \"London\", \"Tokyo\", \"Paris\"]),\n",
    "            country=random.choice([\"the United States\", \"France\", \"Germany\", \"Japan\"]),\n",
    "            years=random.randint(1, 20),\n",
    "            information_type=random.choice([\"confidential\", \"insider\", \"non-public\"]),\n",
    "            regulatory_body=random.choice([\"SEC\", \"FTC\", \"FCA\", \"EU Commission\"]),\n",
    "            confidentiality=random.choice([\"confidential\", \"non-public\", \"privileged\"]),\n",
    "            insider=random.choice([\"insider\", \"internal\", \"secret\"]),\n",
    "            merger_acquisition=random.choice([\"merger\", \"acquisition\"]),\n",
    "            contracts=random.choice([\"contracts\", \"agreements\"]),\n",
    "            no_risk=random.choice([\"no risk\", \"little risk\"]),\n",
    "            risk_free=random.choice([\"risk-free\", \"guaranteed\"]),\n",
    "            high_return=random.choice([\"high returns\", \"massive gains\"]),\n",
    "            promise=random.choice([\"promise\", \"guarantee\"]),\n",
    "            gains=random.choice([\"massive gains\", \"great returns\"]),\n",
    "            no_downside=random.choice([\"no downside\", \"no risk\"])\n",
    "        )\n",
    "\n",
    "        # Add query and its corresponding topic as a tuple to the set\n",
    "        queries_and_labels.add((query, topic))\n",
    "\n",
    "    # Return the unique set of queries as a list\n",
    "    return list(queries_and_labels)\n",
    "\n",
    "# Example templates for generating data\n",
    "COMPLIANCE_RULES = {\n",
    "    \"forward_looking_statements\": [\n",
    "        \"What are {company}'s revenue projections for the {time_period}?\",\n",
    "        \"Will {company}'s stock {action} based on {market_conditions} over the {time_period}?\",\n",
    "        \"Do you expect {company}'s {sector} performance to outpace competitors in {location} over the {time_period}?\",\n",
    "        \"How will {company}'s strategic initiatives impact their earnings event in the {time_period}?\",\n",
    "        \"Is {company} likely to adjust its capital allocation strategy given {market_conditions}?\"\n",
    "    ],\n",
    "    \"insider_information\": [\n",
    "        \"Is there any {information_type} information about {company}'s upcoming {event}?\",\n",
    "        \"Can you share any {insider} insights on {company}'s earnings event for {time_period}?\",\n",
    "        \"What {confidentiality} details can you provide about {company}'s {document} and its impact on their stock price?\",\n",
    "        \"Is there any {confidentiality} information about {company}'s {merger_acquisition} announcement?\",\n",
    "        \"Do you have access to any {insider} reports about {company}'s {contracts} or agreements?\"\n",
    "    ],\n",
    "    \"price_target_inquiry\": [\n",
    "        \"What's your price target for {company}'s stock over {time_period}?\",\n",
    "        \"Do you see {company}'s stock reaching {price} given {market_conditions}?\",\n",
    "        \"What are the expected {sector} returns for {company} by {time_period}?\",\n",
    "        \"Could {company}'s stock hit {price} in the {time_period}?\",\n",
    "        \"What valuation do you anticipate for {company} based on {market_conditions} and {sector} trends?\"\n",
    "    ],\n",
    "    \"mnpi\": [\n",
    "        \"Is there any {insider} data on {company}'s earnings event before its public release?\",\n",
    "        \"Do you have {confidentiality} info on {company}'s {merger_acquisition}?\",\n",
    "        \"Can you provide any {information_type} insights on {company}'s project?\",\n",
    "        \"Are there any material non-public details regarding {company}'s earnings event?\",\n",
    "        \"Can you disclose {company}'s {document} under {insider} access?\"\n",
    "    ],\n",
    "    \"misleading_statements\": [\n",
    "        \"{company}'s stock is guaranteed to {action} over the {time_period}.\",\n",
    "        \"This trade has {no_risk}; {company} is a sure bet.\",\n",
    "        \"I {promise} {company}'s stock will hit {price} in {time_period}.\",\n",
    "        \"There's {no_downside} to investing in {company} right now.\",\n",
    "        \"Investing in {company} is a {risk_free} opportunity.\"\n",
    "    ],\n",
    "    \"over_promising\": [\n",
    "        \"Expect {high_return} from {company} in the {time_period}.\",\n",
    "        \"{company} is set to {action} over the next {time_period}.\",\n",
    "        \"You can't lose with {company}; the stock is poised to {action}.\",\n",
    "        \"{company} will {action} in the {time_period}the market is aligned perfectly.\",\n",
    "        \"This is the perfect moment to get in on {company}'s {event} before they {action}.\"\n",
    "    ],\n",
    "    \"insider_trading_signals\": [\n",
    "        \"Are we in a blackout period for {company}'s stock?\",\n",
    "        \"Is it permissible to trade {company}'s stock during this quiet period?\",\n",
    "        \"When does the blackout period end for {company}?\",\n",
    "        \"Can we execute trades on {company} now, or are we restricted?\",\n",
    "        \"Is the trading window currently closed for {company}?\"\n",
    "    ],\n",
    "    \"nda_violations\": [\n",
    "        \"Are we allowed to disclose {company}'s {document} under the NDA?\",\n",
    "        \"Does the NDA cover {company}'s project, or can we discuss it?\",\n",
    "        \"Is this {document} related to {company} covered under the NDA?\",\n",
    "        \"Are there restrictions on sharing {company}'s {contracts} with external parties?\",\n",
    "        \"Can we talk about {company}'s project, or is it protected by NDA?\"\n",
    "    ],\n",
    "    \"conflict_of_interest\": [\n",
    "        \"Do you have any personal interest in this {merger_acquisition} involving {company}?\",\n",
    "        \"Is there any conflict of interest with {company}'s deal that needs disclosure?\",\n",
    "        \"Does {company}'s project pose any potential conflict of interest?\",\n",
    "        \"Are there any undisclosed relationships that could affect {company}'s transaction?\",\n",
    "        \"Is there a risk of bias in {company}'s {event} due to a conflict of interest?\"\n",
    "    ],\n",
    "    \"fiduciary_breach\": [\n",
    "        \"Has {company}'s trustee breached their fiduciary duty?\",\n",
    "        \"Is {company} involved in a breach of fiduciary trust with this project?\",\n",
    "        \"Could {company} face legal action for fiduciary mismanagement?\",\n",
    "        \"Is there evidence that {company} has violated fiduciary obligations?\",\n",
    "        \"Do you believe {company}'s actions are a breach of fiduciary duty?\"\n",
    "    ],\n",
    "    \"high_risk_language\": [\n",
    "        \"{company}'s stock is highly volatile but could provide {high_return}.\",\n",
    "        \"If you're willing to take a chance, {company}'s stock could {action}.\",\n",
    "        \"{company} is a high-risk, high-reward play given {market_conditions}.\",\n",
    "        \"This is a speculative investment on {company}, but the upside is massive.\",\n",
    "        \"Investing in {company} comes with high risk language, but the potential rewards are great.\"\n",
    "    ],\n",
    "    \"unauthorized_disclosures\": [\n",
    "        \"Are we authorized to share {company}'s {document} externally?\",\n",
    "        \"Can we disclose {company}'s {document}, or is it restricted?\",\n",
    "        \"Is {company}'s {event} considered confidential, or can we discuss it?\",\n",
    "        \"Are we allowed to distribute {company}'s {document} before it's official?\",\n",
    "        \"Is {company}'s {document} ready for public release, or is it still restricted?\"\n",
    "    ],\n",
    "    \"investment_advice_without_disclaimer\": [\n",
    "        \"I strongly advise buying {company}'s stock now.\",\n",
    "        \"You should invest in {company}'s stock before the {event} is announced.\",\n",
    "        \"Sell {company}'s stock before {market_conditions} change.\",\n",
    "        \"Increase your position in {company}'s stock; {high_return} is expected.\",\n",
    "        \"This is a {risk_free} opportunity to invest in {company}.\"\n",
    "    ],\n",
    "    \"regulatory_speculation\": [\n",
    "        \"Will {company} face regulatory penalties from the {regulatory_body}?\",\n",
    "        \"Do you expect {company} to be fined by the {regulatory_body} due to activity?\",\n",
    "        \"Could the {regulatory_body} launch an investigation into {company}?\",\n",
    "        \"Is there a chance {company}'s actions could result in sanctions by the {regulatory_body}?\",\n",
    "        \"Do you think {company} will face enforcement action from the {regulatory_body}?\"\n",
    "    ],\n",
    "    \"legal_violations\": [\n",
    "        \"Is {company}'s activity illegal under current regulations?\",\n",
    "        \"Could {company} be involved in legal violations related to their project?\",\n",
    "        \"Do you think {company}'s {action} could be considered unlawful?\",\n",
    "        \"Could {company} face litigation due to activity in the {sector}?\",\n",
    "        \"Is there any risk of legal action against {company} for activity?\"\n",
    "    ],\n",
    "    \"market_manipulation\": [\n",
    "        \"Is {company} involved in any market manipulation tactics like spoofing?\",\n",
    "        \"Do you think the recent activity in {company}'s stock is a pump-and-dump?\",\n",
    "        \"Is {company} engaging in price manipulation, or is this just market movement?\",\n",
    "        \"Could {company}'s actions be considered wash trading or market manipulation?\",\n",
    "        \"Is there concern that {company} is influencing the market in the {sector}?\"\n",
    "    ],\n",
    "    \"unapproved_marketing\": [\n",
    "        \"Has {company}'s marketing campaign been approved for release?\",\n",
    "        \"Are we allowed to distribute {company}'s advertisement, or is it unapproved?\",\n",
    "        \"Is {company}'s marketing material ready for public view?\",\n",
    "        \"Can we release {company}'s marketing campaign, or does it need more vetting?\",\n",
    "        \"Is this an unapproved draft of {company}'s campaign?\"\n",
    "    ],\n",
    "    \"weather\": [\n",
    "        \"How will weather in {location} affect {company}'s {event}?\",\n",
    "        \"Does {company}'s stock depend on weather patterns in {location}?\",\n",
    "        \"Will {location}'s hurricane season impact {company}'s {sector} performance?\",\n",
    "        \"How does extreme weather in {country} affect {company}'s supply chain?\",\n",
    "        \"Will weather changes in {country} affect {company}'s stock?\"\n",
    "    ],\n",
    "    \"general\": [\n",
    "        \"What are {company}'s key products?\",\n",
    "        \"How does {company}'s leadership affect stock performance?\",\n",
    "        \"How has {company}'s growth strategy evolved over the past {years} years?\",\n",
    "        \"What is {company}'s position in the {sector} market?\",\n",
    "        \"What impact will {company}'s new product launch have on the {sector} market?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate a dataset of 1000 queries with compliance rule labels\n",
    "training_queries_with_labels = generate_data_with_labels(COMPLIANCE_RULES, num_samples=10)\n",
    "print(training_queries_with_labels)\n",
    "# Build the Aho-Corasick automaton using the generated data\n",
    "automaton = build_aho_corasick_automaton_from_labels(training_queries_with_labels)\n",
    "\n",
    "# Function to classify a query's topic using a trained model (logistic regression)\n",
    "def classify_topic(query, clf, vectorizer):\n",
    "    X_query = vectorizer.transform([query])\n",
    "    predicted_topic = clf.predict(X_query)\n",
    "    return predicted_topic[0]\n",
    "\n",
    "# Train a simple logistic regression model for topic classification\n",
    "def train_topic_classifier(queries_with_labels):\n",
    "    queries = [item[0] for item in queries_with_labels]\n",
    "    labels = [item[1] for item in queries_with_labels]\n",
    "\n",
    "    # Vectorize the queries\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(queries)\n",
    "\n",
    "    # Train the logistic regression classifier\n",
    "    clf = LogisticRegression().fit(X_train, labels)\n",
    "    return clf, vectorizer\n",
    "\n",
    "# Train the classifier\n",
    "clf, vectorizer = train_topic_classifier(training_queries_with_labels)\n",
    "\n",
    "# Comprehensive compliance check function\n",
    "def check_compliance(query, automaton, clf, vectorizer):\n",
    "    sentences = sent_tokenize(query)\n",
    "\n",
    "    # Step 1: Use Aho-Corasick to detect compliance issues\n",
    "    # compliance_issues = detect_non_compliance_aho_corasick(automaton, query)\n",
    "\n",
    "    non_compliant_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not check_sentence_compliance(sentence, automaton):\n",
    "            non_compliant_sentences.append(sentence)\n",
    "\n",
    "    # Step 2: Classify the topic using the logistic regression classifier\n",
    "    topic = classify_topic(query, clf, vectorizer)\n",
    "\n",
    "    c_label = ['weather', 'general']\n",
    "    # If the topic is finance/business and we have compliance issues\n",
    "    # if topic not in c_label and compliance_issues:\n",
    "    #     return {\"status\": \"Non-Compliant\", \"rules\": compliance_issues, \"topic\": topic}\n",
    "    \n",
    "    # return {\"status\": \"Compliant\", \"topic\": topic}\n",
    "\n",
    "    # Determine overall compliance\n",
    "    if topic not in c_label and non_compliant_sentences:\n",
    "        return {\n",
    "            \"status\": \"Non-Compliant\",\n",
    "            \"non_compliant_sentences\": non_compliant_sentences,\n",
    "            \"topic\": topic\n",
    "        }\n",
    "        \n",
    "    return {\"status\": \"Compliant\", \"topic\": topic}\n",
    "\n",
    "# Example query to check compliance\n",
    "query = \"what's novonordiks price?\"\n",
    "result = check_compliance(query, automaton, clf, vectorizer)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'Non-Compliant', 'non_compliant_sentences': ['In this example, we will consider a dictionary consisting of the following words: {a, ab, bab, bc, bca, c, caa}.', 'The graph below is the Aho Corasick data structure constructed from the specified dictionary, with each row in the table representing a node in the trie, with the column path indicating the (unique) sequence of characters from the root to the node.', 'The data structure has one node for every prefix of every string in the dictionary.', 'So if (bca) is in the dictionary, then there will be nodes for (bca), (bc), (b), and ().', 'If a node is in the dictionary then it is a blue node.', 'Otherwise it is a grey node.', \"There is a black directed 'child' arc from each node to a node whose name is found by appending one character.\", 'So there is a black arc from (bc) to (bca).', \"There is a blue directed 'suffix' arc from each node to the node that is the longest possible strict suffix of it in the graph.\", 'For example, for node (caa), its strict suffixes are (aa) and (a) and ().', 'The longest of these that exists in the graph is (a).', 'So there is a blue arc from (caa) to (a).', 'The blue arcs can be computed in linear time by performing a breadth-first search [potential suffix node will always be at lower level] starting from the root.', \"The target for the blue arc of a visited node can be found by following its parent's blue arc to its longest suffix node and searching for a child of the suffix node whose character matches that of the visited node.\", 'If the character does not exist as a child, we can find the next longest suffix (following the blue arc again) and then search for the character.', 'We can do this until we either find the character (as child of a node) or we reach the root (which will always be a suffix of every string).', \"There is a green 'dictionary suffix' arc from each node to the next node in the dictionary that can be reached by following blue arcs.\", 'For example, there is a green arc from (bca) to (a) because (a) is the first node in the dictionary (i.e.', 'a blue node) that is reached when following the blue arcs to (ca) and then on to (a).', 'The green arcs can be computed in linear time by repeatedly traversing blue arcs until a blue node is found, and memoizing this information.'], 'topic': np.str_('nda_violations')}\n"
     ]
    }
   ],
   "source": [
    "query = \"In this example, we will consider a dictionary consisting of the following words: {a, ab, bab, bc, bca, c, caa}. The graph below is the Aho Corasick data structure constructed from the specified dictionary, with each row in the table representing a node in the trie, with the column path indicating the (unique) sequence of characters from the root to the node. The data structure has one node for every prefix of every string in the dictionary. So if (bca) is in the dictionary, then there will be nodes for (bca), (bc), (b), and (). If a node is in the dictionary then it is a blue node. Otherwise it is a grey node. There is a black directed 'child' arc from each node to a node whose name is found by appending one character. So there is a black arc from (bc) to (bca). There is a blue directed 'suffix' arc from each node to the node that is the longest possible strict suffix of it in the graph. For example, for node (caa), its strict suffixes are (aa) and (a) and (). The longest of these that exists in the graph is (a). So there is a blue arc from (caa) to (a). The blue arcs can be computed in linear time by performing a breadth-first search [potential suffix node will always be at lower level] starting from the root. The target for the blue arc of a visited node can be found by following its parent's blue arc to its longest suffix node and searching for a child of the suffix node whose character matches that of the visited node. If the character does not exist as a child, we can find the next longest suffix (following the blue arc again) and then search for the character. We can do this until we either find the character (as child of a node) or we reach the root (which will always be a suffix of every string). There is a green 'dictionary suffix' arc from each node to the next node in the dictionary that can be reached by following blue arcs. For example, there is a green arc from (bca) to (a) because (a) is the first node in the dictionary (i.e. a blue node) that is reached when following the blue arcs to (ca) and then on to (a). The green arcs can be computed in linear time by repeatedly traversing blue arcs until a blue node is found, and memoizing this information. \"\n",
    "result = check_compliance(query, automaton, clf, vectorizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compliance_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
