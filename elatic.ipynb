{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import AsyncElasticsearch, exceptions, helpers\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.parse import urlparse\n",
    "\n",
    "# # Provided Elasticsearch URL\n",
    "# es_url = \"https://elastic:NF7Ne7F691OSKwJNcGDNLQjw@04ae44a42e8e437abd819f24a3cd1015.eastus2.azure.elastic-cloud.com:433\"\n",
    "\n",
    "# Parse the URL\n",
    "# parsed_url = urlparse(es_url)\n",
    "\n",
    "# Extract components\n",
    "# scheme = parsed_url.scheme\n",
    "# host = parsed_url.hostname\n",
    "# port = parsed_url.port\n",
    "# username = parsed_url.username\n",
    "# password = parsed_url.password\n",
    "# https://test-176839.es.canadacentral.azure.elastic-cloud.com\n",
    "# Initialize Elasticsearch client\n",
    "es = AsyncElasticsearch(\"https://test-176839.es.canadacentral.azure.elastic-cloud.com:443/\", basic_auth=('elastic', 'T0eZm4Dg4qDP72ZlshNNsyC2'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object AsyncElasticsearch.index at 0x14470e6c0>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = {\n",
    "    \"sanitiztion_term\": \"example term\",\n",
    "    \"entitiy\": \"example entity\"\n",
    "}\n",
    "\n",
    "es.index(index='regex_rules_index', body=sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'emp_id': '12345678',\n",
       "  'session_id': '55439421-fc1a-4f29-91c3-ee956e512abf',\n",
       "  'message_id': 'b3115cfe-ca1e-4897-8169-69c5dc7591ae',\n",
       "  'input': 'top 5 ways to master javascript',\n",
       "  'timestamp': datetime.datetime(2024, 9, 12, 1, 1, 1),\n",
       "  'feedback_rating_timestamp': datetime.datetime(2024, 9, 12, 1, 2, 2),\n",
       "  'output': 'Mastering JavaScript can be a rewarding journey! Here are the top 5 ways to get there:\\n\\n1. **Self-Guided Websites and Courses**: Platforms like [freeCodeCamp](https://www.freecodecamp.org/), [Codecademy](https://www.codecademy.com/), and [Udemy](https://www.udemy.com/) offer comprehensive JavaScript courses that you can follow at your own pace³.\\n\\n2. **Books**: Reading books like \"Eloquent JavaScript\" by Marijn Haverbeke and \"JavaScript: The Good Parts\" by Douglas Crockford can provide deep insights into the language³.\\n\\n3. **Coding Boot Camps**: Intensive coding boot camps, such as those offered by [Fullstack Academy](https://www.fullstackacademy.com/) and [General Assembly](https://generalassemb.ly/), can accelerate your learning with structured, immersive programs³.\\n\\n4. **Meetups and Networking Events**: Engaging with the JavaScript community through meetups, conferences, and online forums like Stack Overflow can help you learn from others and stay updated with the latest trends³.\\n\\n5. **Starting Your Own Projects**: Building your own projects is one of the best ways to apply what you\\'ve learned. Start with small projects and gradually take on more complex ones to solidify your understanding³.\\n\\nWhich of these methods do you think would work best for you?\\n\\nSource: Conversation with Copilot, 2024-09-12\\n(1) The 5 Best Ways to Learn JavaScript Fast (For Beginners). https://techbootcamps.utexas.edu/blog/best-ways-to-learn-javascript/.\\n(2) The best ways to master JavaScript - Educative. https://www.educative.io/blog/master-javascript.\\n(3) How to Learn JavaScript Effectively – Tips and Learning Strategies. https://www.freecodecamp.org/news/how-to-learn-javascript-effectively/.\\n(4) 8 Best Ways to Learn JavaScript in 2024 - For Beginners - Fullstack Academy. https://www.fullstackacademy.com/blog/best-ways-to-learn-javascript-for-beginners.',\n",
       "  'feedback_rating': None,\n",
       "  'chat_type': 'text'},\n",
       " {'id': 2,\n",
       "  'emp_id': '12345678',\n",
       "  'session_id': '633656ba-3b7e-496a-a83b-8725087836ae',\n",
       "  'message_id': 'f6cf82db-570c-4770-a4f9-d5f927b3db91',\n",
       "  'input': 'summarize this document for me',\n",
       "  'timestamp': datetime.datetime(2024, 9, 12, 2, 2, 2),\n",
       "  'feedback_rating_timestamp': datetime.datetime(2024, 9, 12, 2, 3, 3),\n",
       "  'output': 'Sure! Here are the top 5 ways to master JavaScript:\\n\\n1. **Online Courses**: Use platforms like freeCodeCamp, Codecademy, and Udemy.\\n2. **Books**: Read \"Eloquent JavaScript\" and \"JavaScript: The Good Parts.\"\\n3. **Boot Camps**: Attend intensive programs like those from Fullstack Academy and General Assembly.\\n4. **Community Engagement**: Join meetups, conferences, and forums.\\n5. **Personal Projects**: Build your own projects to apply your knowledge.\\n\\nWhich method do you find most appealing?',\n",
       "  'feedback_rating': None,\n",
       "  'chat_type': 'file'}]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "# Install psycopg[binary] if not already installed\n",
    "# pip install psycopg[binary]\n",
    "\n",
    "# Get the PostgreSQL URL from environment variables\n",
    "POSTGRES_URL = os.getenv('POSTGRES_URL', 'postgresql://postgres.ecaazettexwdvahwhaen:zG9Dtbm0xuL5xmm4@aws-0-ca-central-1.pooler.supabase.com:6543/postgres')\n",
    "\n",
    "async def fetch_chat_messages():\n",
    "    # Connect to the PostgreSQL database\n",
    "    async with await psycopg.AsyncConnection.connect(POSTGRES_URL) as conn:\n",
    "        async with conn.cursor(row_factory=dict_row) as cur:\n",
    "            # Execute the query\n",
    "            await cur.execute(\"SELECT * FROM chat_messages\")\n",
    "            # Fetch all results\n",
    "            results = await cur.fetchall()\n",
    "            return results\n",
    "\n",
    "# Run the async function\n",
    "await fetch_chat_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'created_at': datetime.datetime(2024, 9, 12, 1, 1, 1),\n",
       "  'active': True,\n",
       "  'rule': 'questions that are forward-looking or predictive are prohibited.'},\n",
       " {'id': 2,\n",
       "  'created_at': datetime.datetime(2024, 9, 12, 2, 2, 2),\n",
       "  'active': True,\n",
       "  'rule': 'questions that ask to generate ratings, price targets, estimates, or similar recommendations are prohibited.'},\n",
       " {'id': 3,\n",
       "  'created_at': datetime.datetime(2024, 9, 12, 3, 3, 3),\n",
       "  'active': True,\n",
       "  'rule': \"questions that include information that is confidential or non-public relating to J.P. Morgan, J.P. Morgan clients, employees, contract workers, third parties, or any other topic are prohibited. Examples include J.P. Morgan's financial information, client and employee personal information, client trade data (live or historical), business plans or strategies, pending product launches, pitchbooks, partnerships or contracts with third parties, and employment relationships.\"}]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "# Install psycopg[binary] if not already installed\n",
    "# pip install psycopg[binary]\n",
    "\n",
    "# Get the PostgreSQL URL from environment variables\n",
    "POSTGRES_URL = os.getenv('POSTGRES_URL', 'postgresql://postgres.ecaazettexwdvahwhaen:zG9Dtbm0xuL5xmm4@aws-0-ca-central-1.pooler.supabase.com:6543/postgres')\n",
    "\n",
    "async def fetch_chat_messages():\n",
    "    # Connect to the PostgreSQL database\n",
    "    async with await psycopg.AsyncConnection.connect(POSTGRES_URL) as conn:\n",
    "        async with conn.cursor(row_factory=dict_row) as cur:\n",
    "            # Execute the query\n",
    "            await cur.execute(\"SELECT * FROM compliance_rules\")\n",
    "            # Fetch all results\n",
    "            results = await cur.fetchall()\n",
    "            return results\n",
    "\n",
    "# Run the async function\n",
    "await fetch_chat_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mr/03247x2x6sj2gbpc28fgw4v00000gn/T/ipykernel_88106/964134472.py:50: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  await es.indices.create(index=index_name, body=body, ignore=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index: regex_rules_index_v2\n",
      "Indexed document 0\n",
      "Indexed document 1\n",
      "Indexed document 2\n",
      "Reindexed data from regex_rules_index to regex_rules_index_v2\n",
      "Updated document 1\n",
      "Total hits: 1\n",
      "Match: {'sanitiztion_term': 'predictive', 'entitiy': 'sec'}, Score: 1.1727304\n"
     ]
    }
   ],
   "source": [
    "index_name = \"regex_rules_index_v2\"  # New index name\n",
    "\n",
    "# Step 1: Create the Index with the Mapping (No Change)\n",
    "async def create_index():\n",
    "    body = {\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"filter\": {\n",
    "                    \"synonym_filter\": {\n",
    "                        \"type\": \"synonym\",\n",
    "                        \"synonyms\": [\"predict, forecast, anticipate\"]\n",
    "                    }\n",
    "                },\n",
    "                \"analyzer\": {\n",
    "                    \"custom_synonym_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\"lowercase\", \"synonym_filter\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"sanitiztion_term\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"fields\": {\n",
    "                        \"stemmed\": {\n",
    "                            \"type\": \"text\",\n",
    "                            \"analyzer\": \"english\"  # Stemming for better match flexibility\n",
    "                        },\n",
    "                        \"synonym\": {\n",
    "                            \"type\": \"text\",\n",
    "                            \"analyzer\": \"custom_synonym_analyzer\"  # Synonyms during indexing\n",
    "                        },\n",
    "                        \"raw\": {\n",
    "                            \"type\": \"keyword\"  # Exact matches\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"entitiy\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"standard\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create index\n",
    "    await es.indices.create(index=index_name, body=body, ignore=400)\n",
    "    print(f\"Created index: {index_name}\")\n",
    "\n",
    "# Step 2: Index Sample Data (No Change)\n",
    "async def load_data():\n",
    "    sample_data = [\n",
    "        {\"sanitiztion_term\": \"predictive\", \"entitiy\": \"sec\"},\n",
    "        {\"sanitiztion_term\": \"forward-looking\", \"entitiy\": \"sec\"},\n",
    "        {\"sanitiztion_term\": \"example term\", \"entitiy\": \"example entity\"},\n",
    "    ]\n",
    "\n",
    "    # Index documents\n",
    "    for i, doc in enumerate(sample_data):\n",
    "        await es.index(index=index_name, id=i, document=doc)\n",
    "        print(f\"Indexed document {i}\")\n",
    "\n",
    "# Step 3: Reindex Data from Old Index (No Change)\n",
    "async def reindex_data():\n",
    "    old_index = \"regex_rules_index\"\n",
    "    new_index = index_name\n",
    "\n",
    "    body = {\n",
    "        \"source\": {\"index\": old_index},\n",
    "        \"dest\": {\"index\": new_index}\n",
    "    }\n",
    "\n",
    "    await es.reindex(body=body, wait_for_completion=True)\n",
    "    print(f\"Reindexed data from {old_index} to {new_index}\")\n",
    "\n",
    "# Step 4: Update Document (No Change)\n",
    "async def update_document(doc_id, new_data):\n",
    "    await es.update(index=index_name, id=doc_id, body={\"doc\": new_data})\n",
    "    print(f\"Updated document {doc_id}\")\n",
    "\n",
    "# Step 5: Query with Query-Time Synonyms and Boosting\n",
    "async def query_with_synonyms(query_text):\n",
    "    body = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query_text,\n",
    "                \"fields\": [\n",
    "                    \"sanitiztion_term^3\",         # Boost sanitization term matches\n",
    "                    \"sanitiztion_term.synonym^2\",  # Synonym boosted, but lower than sanitiztion_term\n",
    "                    \"sanitiztion_term.stemmed\",    # Allow for stemming matches\n",
    "                    \"entitiy\"                      # Allow entity matches too\n",
    "                ],\n",
    "                \"fuzziness\": \"AUTO\",              # Allow for fuzzy matches\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = await es.search(index=index_name, body=body)\n",
    "    hits = response['hits']['hits']\n",
    "\n",
    "    if hits:\n",
    "        print(f\"Total hits: {len(hits)}\")\n",
    "        for hit in hits:\n",
    "            print(f\"Match: {hit['_source']}, Score: {hit['_score']}\")\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "\n",
    "# Step 6: Test Query with \"Can you predict the price of Tesla\"\n",
    "async def test_query():\n",
    "    query_text = \"can you predict the price of Tesla\"\n",
    "    await query_with_synonyms(query_text)\n",
    "\n",
    "# Run all the steps\n",
    "async def main():\n",
    "    await create_index()\n",
    "    await load_data()\n",
    "    await reindex_data()  # Reindex only if needed\n",
    "    await update_document(1, {\"sanitiztion_term\": \"updated term\"})  # Update sample doc\n",
    "    await test_query()  # Test query with synonym\n",
    "\n",
    "# Run the script\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS\n",
    "async def query_with_synonyms(query_text):\n",
    "    body = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                    \"query\": \"With the current trends in global trade and economic policies, how do you anticipate the economic landscape will evolve in the next 20 years, and what measures should countries take to ensure sustainable growth and stability?\",\n",
    "                    \"fields\": [\"sanitization_term\"],\n",
    "                    \"operator\": \"or\",\n",
    "                    \"fuzziness\": \"AUTO\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    response = await es.search(index=index_name, body=body)\n",
    "    hits = response['hits']['hits']\n",
    "\n",
    "    if hits:\n",
    "        print(f\"Total hits: {len(hits)}\")\n",
    "        for hit in hits:\n",
    "            print(f\"Match: {hit['_source']}, Score: {hit['_score']}\")\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "\n",
    "# Step 6: Test Query with \"Can you predict the price of Tesla\"\n",
    "async def test_query():\n",
    "    query_text = \"what's the forward guidance for the price target of Tesla\"\n",
    "    await query_with_synonyms(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches found.\n"
     ]
    }
   ],
   "source": [
    "await test_query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'regex_rules_index' already exists.\n",
      "Index 'regex_rules_index' closed successfully.\n",
      "Settings for index 'regex_rules_index' updated successfully.\n",
      "Index 'regex_rules_index' opened successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x116693a40>\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "BadRequestError(400, 'illegal_argument_exception', 'Mapper for [sanitization_term] conflicts with existing mapper:\\n\\tCannot update parameter [analyzer] from [custom_standard_analyzer] to [default]')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m add_mapping()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Run the script\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "Cell \u001b[0;32mIn[199], line 110\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m add_settings()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m open_index()\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m add_mapping()\n",
      "Cell \u001b[0;32mIn[199], line 99\u001b[0m, in \u001b[0;36madd_mapping\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m mapping_body \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msanitization_term\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     }\n\u001b[1;32m     97\u001b[0m }\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m es\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mexists(index\u001b[38;5;241m=\u001b[39mindex_name):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m es\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mput_mapping(index\u001b[38;5;241m=\u001b[39mindex_name, body\u001b[38;5;241m=\u001b[39mmapping_body)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMapping for index \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m updated successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/elasticsearch/_async/client/indices.py:3133\u001b[0m, in \u001b[0;36mIndicesClient.put_mapping\u001b[0;34m(self, index, allow_no_indices, date_detection, dynamic, dynamic_date_formats, dynamic_templates, error_trace, expand_wildcards, field_names, filter_path, human, ignore_unavailable, master_timeout, meta, numeric_detection, pretty, properties, routing, runtime, source, timeout, write_index_only, body)\u001b[0m\n\u001b[1;32m   3131\u001b[0m         __body[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m source\n\u001b[1;32m   3132\u001b[0m __headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m-> 3133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperform_request(  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n\u001b[1;32m   3134\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3135\u001b[0m     __path,\n\u001b[1;32m   3136\u001b[0m     params\u001b[38;5;241m=\u001b[39m__query,\n\u001b[1;32m   3137\u001b[0m     headers\u001b[38;5;241m=\u001b[39m__headers,\n\u001b[1;32m   3138\u001b[0m     body\u001b[38;5;241m=\u001b[39m__body,\n\u001b[1;32m   3139\u001b[0m     endpoint_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices.put_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3140\u001b[0m     path_parts\u001b[38;5;241m=\u001b[39m__path_parts,\n\u001b[1;32m   3141\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/elasticsearch/_async/client/_base.py:423\u001b[0m, in \u001b[0;36mNamespacedClient.perform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_request\u001b[39m(\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    412\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;66;03m# Use the internal clients .perform_request() implementation\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# so we take advantage of their transport options.\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mperform_request(\n\u001b[1;32m    424\u001b[0m         method,\n\u001b[1;32m    425\u001b[0m         path,\n\u001b[1;32m    426\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    427\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    428\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    429\u001b[0m         endpoint_id\u001b[38;5;241m=\u001b[39mendpoint_id,\n\u001b[1;32m    430\u001b[0m         path_parts\u001b[38;5;241m=\u001b[39mpath_parts,\n\u001b[1;32m    431\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/elasticsearch/_async/client/_base.py:271\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_request\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    257\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     path_parts: Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ApiResponse[Any]:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_otel\u001b[38;5;241m.\u001b[39mspan(\n\u001b[1;32m    267\u001b[0m         method,\n\u001b[1;32m    268\u001b[0m         endpoint_id\u001b[38;5;241m=\u001b[39mendpoint_id,\n\u001b[1;32m    269\u001b[0m         path_parts\u001b[38;5;241m=\u001b[39mpath_parts \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[1;32m    270\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m otel_span:\n\u001b[0;32m--> 271\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform_request(\n\u001b[1;32m    272\u001b[0m             method,\n\u001b[1;32m    273\u001b[0m             path,\n\u001b[1;32m    274\u001b[0m             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    275\u001b[0m             headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    276\u001b[0m             body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    277\u001b[0m             otel_span\u001b[38;5;241m=\u001b[39motel_span,\n\u001b[1;32m    278\u001b[0m         )\n\u001b[1;32m    279\u001b[0m         otel_span\u001b[38;5;241m.\u001b[39mset_elastic_cloud_metadata(response\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/elasticsearch/_async/client/_base.py:352\u001b[0m, in \u001b[0;36mBaseClient._perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[38;5;241m.\u001b[39mget(meta\u001b[38;5;241m.\u001b[39mstatus, ApiError)(\n\u001b[1;32m    353\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage, meta\u001b[38;5;241m=\u001b[39mmeta, body\u001b[38;5;241m=\u001b[39mresp_body\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# 'X-Elastic-Product: Elasticsearch' should be on every 2XX response.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verified_elasticsearch:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# If the header is set we mark the server as verified.\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: BadRequestError(400, 'illegal_argument_exception', 'Mapper for [sanitization_term] conflicts with existing mapper:\\n\\tCannot update parameter [analyzer] from [custom_standard_analyzer] to [default]')"
     ]
    }
   ],
   "source": [
    "# Define the index name - USE THIS\n",
    "index_name = 'regex_rules_index'\n",
    "\n",
    "# Create the index if it does not exist\n",
    "async def create_index():\n",
    "    if not await es.indices.exists(index=index_name):\n",
    "        index_body = {\n",
    "            \"settings\": {\n",
    "                \"analysis\": {\n",
    "                    \"filter\": {\n",
    "                        \"synonym_filter\": {\n",
    "                            \"type\": \"synonym\",\n",
    "                            \"synonyms\": [\"predict, forecast, anticipate\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"analyzer\": {\n",
    "                        \"custom_synonym_analyzer\": {\n",
    "                            \"type\": \"custom\",\n",
    "                            \"tokenizer\": \"standard\",\n",
    "                            \"filter\": [\"lowercase\", \"synonym_filter\"]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        await es.indices.create(index=index_name, body=index_body)\n",
    "        print(f\"Index '{index_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "# Close the index\n",
    "async def close_index():\n",
    "    if await es.indices.exists(index=index_name):\n",
    "        await es.indices.close(index=index_name)\n",
    "        print(f\"Index '{index_name}' closed successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' does not exist to close.\")\n",
    "\n",
    "# Update the index settings\n",
    "async def add_settings():\n",
    "    if await es.indices.exists(index=index_name):\n",
    "        settings_body = {\n",
    "            \"analysis\": {\n",
    "                \"filter\": {\n",
    "                    \"synonym_filter\": {\n",
    "                        \"type\": \"synonym\",\n",
    "                        \"synonyms\": [\"predict, forecast, anticipate\"]\n",
    "                    }\n",
    "                },\n",
    "                \"analyzer\": {\n",
    "                    \"custom_synonym_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\"lowercase\", \"synonym_filter\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        await es.indices.put_settings(index=index_name, body=settings_body)\n",
    "        print(f\"Settings for index '{index_name}' updated successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' does not exist to update settings.\")\n",
    "\n",
    "# Reopen the index\n",
    "async def open_index():\n",
    "    if await es.indices.exists(index=index_name):\n",
    "        await es.indices.open(index=index_name)\n",
    "        print(f\"Index '{index_name}' opened successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' does not exist to open.\")\n",
    "\n",
    "# Update the index mapping\n",
    "async def add_mapping():\n",
    "    mapping_body = {\n",
    "        \"properties\": {\n",
    "            \"sanitization_term\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"stemmed\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"english\"\n",
    "                    },\n",
    "                    \"synonym\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"custom_synonym_analyzer\"\n",
    "                    },\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"entity\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if await es.indices.exists(index=index_name):\n",
    "        await es.indices.put_mapping(index=index_name, body=mapping_body)\n",
    "        print(f\"Mapping for index '{index_name}' updated successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' does not exist to update mapping.\")\n",
    "\n",
    "# Run all the steps\n",
    "async def main():\n",
    "    await create_index()\n",
    "    await close_index()\n",
    "    await add_settings()\n",
    "    await open_index()\n",
    "    await add_mapping()\n",
    "\n",
    "# Run the script\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "\n",
    "    {\"sanitiztion_term\": \"predictive\", \"entitiy\": \"sec\"},\n",
    "    {\"sanitiztion_term\": \"forward-looking\", \"entitiy\": \"sec\"},\n",
    "    {\"sanitiztion_term\": \"example term\", \"entitiy\": \"example entity\"},\n",
    "]\n",
    "\n",
    "# Index documents\n",
    "for i, doc in enumerate(sample_data):\n",
    "    await es.index(index=index_name, id=i, document=doc)\n",
    "    print(f\"Indexed document {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import inflect\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Download required nltk resources\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inflect engine for singular/plural transformations\n",
    "inflector = inflect.engine()\n",
    "\n",
    "# Compliance rules dictionary\n",
    "COMPLIANCE_RULES = {\n",
    "    \"forward_looking_statements\": {\"expect\", \"project\", \"forecast\", \"estimate\", \"anticipate\", \"plan\", \"will\", \"predict\", \"target\", \"could\", \"should\", \"might\", \"potential\", \"projected\"},\n",
    "    \"insider_information\": {\"non-public\", \"confidential\", \"undisclosed\", \"insider\", \"privileged\", \"secret\"},\n",
    "    \"price_target_inquiry\": {\"price target\", \"future value\", \"next quarter\", \"next year\", \"expected price\", \"forecasted price\"},\n",
    "    \"mnpi\": {\"earnings\", \"revenue\", \"profit\", \"loss\", \"financials\", \"guidance\", \"merger\", \"acquisition\", \"divestiture\", \"restructuring\", \"layoffs\", \"contracts\", \"non-public\", \"confidential\", \"undisclosed\", \"not yet announced\"},\n",
    "    \"misleading_statements\": {\"always\", \"never\", \"guarantee\", \"risk-free\", \"no risk\", \"promise\", \"certain\", \"assured\"},\n",
    "    \"over_promising\": {\"skyrocket\", \"double\", \"triple\", \"explode\", \"massive gains\", \"huge returns\", \"can't lose\", \"win big\"},\n",
    "    \"insider_trading_signals\": {\"quiet period\", \"blackout period\", \"trading window\", \"no trading\", \"silent window\"},\n",
    "    \"nda_violations\": {\"NDA\", \"non-disclosure agreement\", \"under contract\", \"binding agreement\"},\n",
    "    \"conflict_of_interest\": {\"conflict of interest\", \"self-dealing\", \"personal interest\", \"undisclosed relationship\", \"bias\", \"favoritism\"},\n",
    "    \"fiduciary_breach\": {\"fiduciary duty\", \"best interest\", \"confidential duty\", \"breach of trust\", \"trustee violation\", \"mismanagement\"},\n",
    "    \"high_risk_language\": {\"leverage\", \"high risk\", \"all in\", \"bet\", \"gamble\", \"penny stocks\", \"junk bonds\", \"crypto\", \"unregulated\", \"volatile\"},\n",
    "    \"unauthorized_disclosures\": {\"not authorized\", \"not allowed\", \"prohibited\", \"restricted\", \"off-limits\", \"forbidden\"},\n",
    "    \"investment_advice_without_disclaimer\": {\"you should invest\", \"buy now\", \"sell now\", \"strong buy\", \"strong sell\", \"must buy\", \"must sell\", \"this stock will\"},\n",
    "    \"regulatory_speculation\": {\"SEC will\", \"regulators will\", \"likely fine\", \"expect sanctions\", \"could face penalties\", \"might be banned\", \"regulation changes\"},\n",
    "    \"legal_violations\": {\"illegal\", \"against the law\", \"prohibited\", \"unauthorized\", \"unethical\", \"fraud\", \"manipulation\", \"scam\", \"tax evasion\"},\n",
    "    \"market_manipulation\": {\"pump and dump\", \"artificially inflate\", \"market making\", \"wash trading\", \"spoofing\", \"cornering the market\", \"price fixing\"},\n",
    "    \"unapproved_marketing\": {\"unapproved\", \"not vetted\", \"not reviewed\", \"draft version\", \"preliminary version\", \"for internal use only\"}\n",
    "}\n",
    "\n",
    "# Function to check if a word is countable and a noun\n",
    "def is_countable_noun(word):\n",
    "    for synset in wn.synsets(word, pos=wn.NOUN):\n",
    "        # Check if the word is a noun and is not related to quantities (non-countable nouns)\n",
    "        if synset.lexname() == 'noun.quantity':\n",
    "            return False  # It's uncountable if related to quantities\n",
    "    return True\n",
    "\n",
    "# Function to get all synonyms using WordNet\n",
    "def get_synonyms(term):\n",
    "    synonyms = set()\n",
    "    for synset in wn.synsets(term):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))  # Replace underscores with spaces in multi-word terms\n",
    "    return synonyms\n",
    "\n",
    "# Function to generate inflections, but skip invalid pluralization for non-nouns\n",
    "def get_inflections(term):\n",
    "    inflections = set()\n",
    "    inflections.add(term)\n",
    "\n",
    "    # If the term is a multi-word phrase\n",
    "    if ' ' in term:\n",
    "        words = term.split()\n",
    "        last_word = words[-1]\n",
    "\n",
    "        # Check if the last word is a countable noun\n",
    "        if is_countable_noun(last_word):\n",
    "            plural_last_word = inflector.plural(last_word)\n",
    "            singular_last_word = inflector.singular_noun(last_word) or last_word\n",
    "\n",
    "            # Add plural and singular versions of the phrase\n",
    "            inflections.add(' '.join(words[:-1] + [plural_last_word]))\n",
    "            inflections.add(' '.join(words[:-1] + [singular_last_word]))\n",
    "    else:\n",
    "        # If it's a single word, handle singular/plural forms\n",
    "        plural_form = inflector.plural(term)\n",
    "        singular_form = inflector.singular_noun(term) or term\n",
    "\n",
    "        inflections.add(plural_form)\n",
    "        inflections.add(singular_form)\n",
    "\n",
    "        # Adjective forms using WordNet\n",
    "        for synset in wn.synsets(term):\n",
    "            for lemma in synset.lemmas():\n",
    "                derivationally_related_forms = lemma.derivationally_related_forms()\n",
    "                for related_lemma in derivationally_related_forms:\n",
    "                    related_word = related_lemma.name().replace('_', ' ')\n",
    "                    inflections.add(related_word)\n",
    "\n",
    "    return inflections\n",
    "\n",
    "# Function to sanitize and deduplicate terms\n",
    "def sanitize_terms(terms):\n",
    "    sanitized = set()\n",
    "    for term in terms:\n",
    "        sanitized.add(term.lower())  # Basic sanitation: lowercasing for uniformity\n",
    "    return sanitized\n",
    "\n",
    "# Final compliance terms list\n",
    "compliance_terms = []\n",
    "\n",
    "# Generate terms for each compliance rule\n",
    "for rule, terms in COMPLIANCE_RULES.items():\n",
    "    for term in terms:\n",
    "        # Get inflections and synonyms for each term\n",
    "        inflected_terms = get_inflections(term)\n",
    "        synonym_terms = set()\n",
    "        for inflection in inflected_terms:\n",
    "            synonym_terms.update(get_synonyms(inflection))\n",
    "\n",
    "        # Combine all terms (original, inflected, and synonyms)\n",
    "        all_terms = sanitize_terms(inflected_terms.union(synonym_terms))\n",
    "\n",
    "        # Append to final output, mapping each term to the rule\n",
    "        for sanitized_term in all_terms:\n",
    "            compliance_terms.append({\"sanitiztion_term\": sanitized_term, \"entity\": rule})\n",
    "\n",
    "# Display the result\n",
    "compliance_terms_to_load = compliance_terms\n",
    "print(compliance_terms_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS VERSION TWO WITH DEDUPLICATION\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import inflect\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Download required nltk resources\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inflect engine for singular/plural transformations\n",
    "inflector = inflect.engine()\n",
    "\n",
    "# Compliance rules dictionary\n",
    "COMPLIANCE_RULES = {\n",
    "    \"forward_looking_statements\": {\"expect\", \"project\", \"forecast\", \"estimate\", \"anticipate\", \"plan\", \"will\", \"predict\", \"target\", \"could\", \"should\", \"might\", \"potential\", \"projected\"},\n",
    "    \"insider_information\": {\"non-public\", \"confidential\", \"undisclosed\", \"insider\", \"privileged\", \"secret\"},\n",
    "    \"price_target_inquiry\": {\"price target\", \"future value\", \"next quarter\", \"next year\", \"expected price\", \"forecasted price\"},\n",
    "    \"mnpi\": {\"earnings\", \"revenue\", \"profit\", \"loss\", \"financials\", \"guidance\", \"merger\", \"acquisition\", \"divestiture\", \"restructuring\", \"layoffs\", \"contracts\", \"non-public\", \"confidential\", \"undisclosed\", \"not yet announced\"},\n",
    "    \"misleading_statements\": {\"always\", \"never\", \"guarantee\", \"risk-free\", \"no risk\", \"promise\", \"certain\", \"assured\"},\n",
    "    \"over_promising\": {\"skyrocket\", \"double\", \"triple\", \"explode\", \"massive gains\", \"huge returns\", \"can't lose\", \"win big\"},\n",
    "    \"insider_trading_signals\": {\"quiet period\", \"blackout period\", \"trading window\", \"no trading\", \"silent window\"},\n",
    "    \"nda_violations\": {\"NDA\", \"non-disclosure agreement\", \"under contract\", \"binding agreement\"},\n",
    "    \"conflict_of_interest\": {\"conflict of interest\", \"self-dealing\", \"personal interest\", \"undisclosed relationship\", \"bias\", \"favoritism\"},\n",
    "    \"fiduciary_breach\": {\"fiduciary duty\", \"best interest\", \"confidential duty\", \"breach of trust\", \"trustee violation\", \"mismanagement\"},\n",
    "    \"high_risk_language\": {\"leverage\", \"high risk\", \"all in\", \"bet\", \"gamble\", \"penny stocks\", \"junk bonds\", \"crypto\", \"unregulated\", \"volatile\"},\n",
    "    \"unauthorized_disclosures\": {\"not authorized\", \"not allowed\", \"prohibited\", \"restricted\", \"off-limits\", \"forbidden\"},\n",
    "    \"investment_advice_without_disclaimer\": {\"you should invest\", \"buy now\", \"sell now\", \"strong buy\", \"strong sell\", \"must buy\", \"must sell\", \"this stock will\"},\n",
    "    \"regulatory_speculation\": {\"SEC will\", \"regulators will\", \"likely fine\", \"expect sanctions\", \"could face penalties\", \"might be banned\", \"regulation changes\"},\n",
    "    \"legal_violations\": {\"illegal\", \"against the law\", \"prohibited\", \"unauthorized\", \"unethical\", \"fraud\", \"manipulation\", \"scam\", \"tax evasion\"},\n",
    "    \"market_manipulation\": {\"pump and dump\", \"artificially inflate\", \"market making\", \"wash trading\", \"spoofing\", \"cornering the market\", \"price fixing\"},\n",
    "    \"unapproved_marketing\": {\"unapproved\", \"not vetted\", \"not reviewed\", \"draft version\", \"preliminary version\", \"for internal use only\"}\n",
    "}\n",
    "\n",
    "# Function to check if a word is countable and a noun\n",
    "def is_countable_noun(word):\n",
    "    for synset in wn.synsets(word, pos=wn.NOUN):\n",
    "        # Check if the word is a noun and is not related to quantities (non-countable nouns)\n",
    "        if synset.lexname() == 'noun.quantity':\n",
    "            return False  # It's uncountable if related to quantities\n",
    "    return True\n",
    "\n",
    "# Function to get all synonyms using WordNet\n",
    "def get_synonyms(term):\n",
    "    synonyms = set()\n",
    "    for synset in wn.synsets(term):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name().replace('_', ' '))  # Replace underscores with spaces in multi-word terms\n",
    "    return synonyms\n",
    "\n",
    "# Function to generate inflections, but skip invalid pluralization for non-nouns\n",
    "def get_inflections(term):\n",
    "    inflections = set()\n",
    "    inflections.add(term)\n",
    "\n",
    "    # If the term is a multi-word phrase\n",
    "    if ' ' in term:\n",
    "        words = term.split()\n",
    "        last_word = words[-1]\n",
    "\n",
    "        # Check if the last word is a countable noun\n",
    "        if is_countable_noun(last_word):\n",
    "            plural_last_word = inflector.plural(last_word)\n",
    "            singular_last_word = inflector.singular_noun(last_word) or last_word\n",
    "\n",
    "            # Add plural and singular versions of the phrase\n",
    "            inflections.add(' '.join(words[:-1] + [plural_last_word]))\n",
    "            inflections.add(' '.join(words[:-1] + [singular_last_word]))\n",
    "    else:\n",
    "        # If it's a single word, handle singular/plural forms\n",
    "        plural_form = inflector.plural(term)\n",
    "        singular_form = inflector.singular_noun(term) or term\n",
    "\n",
    "        inflections.add(plural_form)\n",
    "        inflections.add(singular_form)\n",
    "\n",
    "        # Adjective forms using WordNet\n",
    "        for synset in wn.synsets(term):\n",
    "            for lemma in synset.lemmas():\n",
    "                derivationally_related_forms = lemma.derivationally_related_forms()\n",
    "                for related_lemma in derivationally_related_forms:\n",
    "                    related_word = related_lemma.name().replace('_', ' ')\n",
    "                    inflections.add(related_word)\n",
    "\n",
    "    return inflections\n",
    "\n",
    "# Function to sanitize and deduplicate terms\n",
    "def sanitize_terms(terms):\n",
    "    sanitized = set()\n",
    "    for term in terms:\n",
    "        sanitized.add(term.lower())  # Basic sanitation: lowercasing for uniformity\n",
    "    return sanitized\n",
    "\n",
    "# Final compliance terms dictionary\n",
    "compliance_terms_dict = {}\n",
    "\n",
    "# Generate terms for each compliance rule\n",
    "for rule, terms in COMPLIANCE_RULES.items():\n",
    "    for term in terms:\n",
    "        # Get inflections and synonyms for each term\n",
    "        inflected_terms = get_inflections(term)\n",
    "        synonym_terms = set()\n",
    "        for inflection in inflected_terms:\n",
    "            synonym_terms.update(get_synonyms(inflection))\n",
    "\n",
    "        # Combine all terms (original, inflected, and synonyms)\n",
    "        all_terms = sanitize_terms(inflected_terms.union(synonym_terms))\n",
    "\n",
    "        # Append to final output, mapping each term to the rule\n",
    "        for sanitized_term in all_terms:\n",
    "            if sanitized_term not in compliance_terms_dict:\n",
    "                compliance_terms_dict[sanitized_term] = set()\n",
    "            compliance_terms_dict[sanitized_term].add(rule)\n",
    "\n",
    "# Convert the dictionary to the desired list format\n",
    "compliance_terms_to_load = [{\"sanitization_term\": term, \"entity\": list(entities)} for term, entities in compliance_terms_dict.items()]\n",
    "\n",
    "# Display the result\n",
    "print(compliance_terms_to_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLIANCE_RULES = {\n",
    "    \"forward_looking_statements\": {\"expect\", \"project\", \"forecast\", \"estimate\", \"anticipate\", \"plan\", \"will\", \"predict\", \"target\", \"could\", \"should\", \"might\", \"potential\", \"projected\"},\n",
    "    \"insider_information\": {\"non-public\", \"confidential\", \"undisclosed\", \"insider\", \"privileged\", \"secret\"},\n",
    "    \"price_target_inquiry\": {\"price target\", \"future value\", \"next quarter\", \"next year\", \"expected price\", \"forecasted price\"},\n",
    "    \"mnpi\": {\"earnings\", \"revenue\", \"profit\", \"loss\", \"financials\", \"guidance\", \"merger\", \"acquisition\", \"divestiture\", \"restructuring\", \"layoffs\", \"contracts\", \"non-public\", \"confidential\", \"undisclosed\", \"not yet announced\"},\n",
    "    \"misleading_statements\": {\"always\", \"never\", \"guarantee\", \"risk-free\", \"no risk\", \"promise\", \"certain\", \"assured\"},\n",
    "    \"over_promising\": {\"skyrocket\", \"double\", \"triple\", \"explode\", \"massive gains\", \"huge returns\", \"can't lose\", \"win big\"},\n",
    "    \"insider_trading_signals\": {\"quiet period\", \"blackout period\", \"trading window\", \"no trading\", \"silent window\"},\n",
    "    \"nda_violations\": {\"NDA\", \"non-disclosure agreement\", \"under contract\", \"binding agreement\"},\n",
    "    \"conflict_of_interest\": {\"conflict of interest\", \"self-dealing\", \"personal interest\", \"undisclosed relationship\", \"bias\", \"favoritism\"},\n",
    "    \"fiduciary_breach\": {\"fiduciary duty\", \"best interest\", \"confidential duty\", \"breach of trust\", \"trustee violation\", \"mismanagement\"},\n",
    "    \"high_risk_language\": {\"leverage\", \"high risk\", \"all in\", \"bet\", \"gamble\", \"penny stocks\", \"junk bonds\", \"crypto\", \"unregulated\", \"volatile\"},\n",
    "    \"unauthorized_disclosures\": {\"not authorized\", \"not allowed\", \"prohibited\", \"restricted\", \"off-limits\", \"forbidden\"},\n",
    "    \"investment_advice_without_disclaimer\": {\"you should invest\", \"buy now\", \"sell now\", \"strong buy\", \"strong sell\", \"must buy\", \"must sell\", \"this stock will\"},\n",
    "    \"regulatory_speculation\": {\"SEC will\", \"regulators will\", \"likely fine\", \"expect sanctions\", \"could face penalties\", \"might be banned\", \"regulation changes\"},\n",
    "    \"legal_violations\": {\"illegal\", \"against the law\", \"prohibited\", \"unauthorized\", \"unethical\", \"fraud\", \"manipulation\", \"scam\", \"tax evasion\"},\n",
    "    \"market_manipulation\": {\"pump and dump\", \"artificially inflate\", \"market making\", \"wash trading\", \"spoofing\", \"cornering the market\", \"price fixing\"},\n",
    "    \"unapproved_marketing\": {\"unapproved\", \"not vetted\", \"not reviewed\", \"draft version\", \"preliminary version\", \"for internal use only\"}\n",
    "}\n",
    "\n",
    "data = []\n",
    "for category, terms in COMPLIANCE_RULES.items():\n",
    "    for term in terms:\n",
    "        data.append({\n",
    "            \"sanitization_term\": term,\n",
    "            \"entity\": category\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def bulk_index(client, index_name, data):\n",
    "    # Prepare the actions for bulk indexing\n",
    "    actions = [\n",
    "        {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": item\n",
    "        }\n",
    "        for item in data\n",
    "    ]\n",
    "\n",
    "    # Use the helpers.bulk method for bulk indexing\n",
    "    success, failed = await helpers.async_bulk(client, actions)\n",
    "    print(f\"Successfully indexed {success} documents\")\n",
    "    print(f\"Failed to index {failed} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 132 documents\n",
      "Failed to index [] documents\n"
     ]
    }
   ],
   "source": [
    "index_name = 'regex_rules_index'\n",
    "await bulk_index(es, index_name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_check = \"forecast\"\n",
    "\n",
    "# Check if the word exists in any of the dictionaries\n",
    "if any(word_to_check in d.values() for d in compliance_terms_to_load):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = False\n",
    "for d in compliance_terms_to_load:\n",
    "    if word_to_check in d.values():\n",
    "        print(f\"Found in dictionary: {d}\")\n",
    "        found = True\n",
    "        break\n",
    "\n",
    "if not found:\n",
    "    print(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_likely_broken_rules(hits, query_length, base_threshold=1.0, query_length_factor=0.01, tolerance_factor=0.05, epsilon=0.0001, top_n=100):\n",
    "    broken_rules = []\n",
    "    counter = 0\n",
    "\n",
    "    if not hits:\n",
    "        return {\"message\": \"No matches found.\"}  # No hits found\n",
    "    \n",
    "    # Calculate dynamic min score threshold based on query length\n",
    "    min_score_threshold = base_threshold - (query_length * query_length_factor)\n",
    "    \n",
    "    # Process only top N results for performance reasons\n",
    "    limited_hits = hits[:top_n]\n",
    "    \n",
    "    # Initialize the previous score with the first hit if it meets the threshold\n",
    "    first_hit = limited_hits[0]\n",
    "    previous_score = first_hit['_score']\n",
    "    \n",
    "    if previous_score >= min_score_threshold:\n",
    "        broken_rules.append(first_hit)\n",
    "        counter += 1\n",
    "    \n",
    "    # Iterate over remaining results\n",
    "    for hit in limited_hits[1:]:\n",
    "        current_score = hit['_score']\n",
    "        \n",
    "        # Check if score meets the dynamically calculated minimum threshold\n",
    "        if current_score >= min_score_threshold:\n",
    "            \n",
    "            # Calculate the adaptive score threshold as a percentage of the previous score\n",
    "            adaptive_threshold = previous_score * tolerance_factor\n",
    "            \n",
    "            # Check if the difference between the current and previous score is within the adaptive threshold\n",
    "            if abs(current_score - previous_score) <= adaptive_threshold or abs(current_score - previous_score) < epsilon:\n",
    "                \n",
    "                # Add the result to the broken rules list\n",
    "                broken_rules.append(hit)\n",
    "                counter += 1\n",
    "                previous_score = current_score  # Update previous score\n",
    "\n",
    "    # If no results meet the threshold, provide a failover\n",
    "    if counter == 0:\n",
    "        return {\"message\": \"No likely broken rules found with current thresholds. Consider broadening criteria.\"}\n",
    "    \n",
    "    return broken_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_likely_broken_rules(hits, query_length, base_threshold=1.0, query_length_factor=0.01, tolerance_factor=0.05, epsilon=0.0001, top_n=100):\n",
    "    \"\"\"\n",
    "    Find the most likely broken rules based on the given hits and query length.\n",
    "\n",
    "    Parameters:\n",
    "    - hits: List of search results with '_score' attributes.\n",
    "    - query_length: Length of the search query.\n",
    "    - base_threshold: Base minimum score threshold.\n",
    "    - query_length_factor: Factor to adjust the threshold based on query length.\n",
    "    - tolerance_factor: Factor to determine the adaptive score difference.\n",
    "    - epsilon: Small value to handle score plateaus.\n",
    "    - top_n: Limit the number of results to process for performance.\n",
    "\n",
    "    Returns:\n",
    "    - List of most likely broken rules or a message if no significant results are found.\n",
    "    \"\"\"\n",
    "    broken_rules = []\n",
    "    counter = 0\n",
    "\n",
    "    if not hits:\n",
    "        return {\"message\": \"No matches found.\"}  # No hits found\n",
    "    \n",
    "    # Calculate dynamic min score threshold based on query length\n",
    "    min_score_threshold = base_threshold - (query_length * query_length_factor)\n",
    "    \n",
    "    # Process only top N results for performance reasons\n",
    "    limited_hits = hits[:top_n]\n",
    "    \n",
    "    # Initialize the previous score with the first hit if it meets the threshold\n",
    "    first_hit = limited_hits[0]\n",
    "    previous_score = first_hit['_score']\n",
    "    \n",
    "    if previous_score >= min_score_threshold:\n",
    "        broken_rules.append(first_hit)\n",
    "        counter += 1\n",
    "    \n",
    "    # Iterate over remaining results\n",
    "    for hit in limited_hits[1:]:\n",
    "        current_score = hit['_score']\n",
    "        \n",
    "        # Check if score meets the dynamically calculated minimum threshold\n",
    "        if current_score >= min_score_threshold:\n",
    "            \n",
    "            # Calculate the adaptive score threshold as a percentage of the previous score\n",
    "            adaptive_threshold = previous_score * tolerance_factor\n",
    "            \n",
    "            # Check if the difference between the current and previous score is within the adaptive threshold\n",
    "            if abs(current_score - previous_score) <= adaptive_threshold or abs(current_score - previous_score) < epsilon:\n",
    "                \n",
    "                # Add the result to the broken rules list\n",
    "                broken_rules.append(hit)\n",
    "                counter += 1\n",
    "                previous_score = current_score  # Update previous score\n",
    "\n",
    "    # If no results meet the threshold, provide a failover\n",
    "    if counter == 0:\n",
    "        return {\"message\": \"No likely broken rules found with current thresholds. Consider broadening criteria.\"}\n",
    "    \n",
    "    return broken_rules\n",
    "\n",
    "# Example usage\n",
    "hits = [\n",
    "    {'_score': 1.5, '_source': {'rule': 'Rule 1'}},\n",
    "    {'_score': 1.4, '_source': {'rule': 'Rule 2'}},\n",
    "    {'_score': 1.35, '_source': {'rule': 'Rule 3'}},\n",
    "    {'_score': 1.1, '_source': {'rule': 'Rule 4'}},\n",
    "    {'_score': 0.9, '_source': {'rule': 'Rule 5'}}\n",
    "]\n",
    "\n",
    "query_length = 10  # Example query length\n",
    "\n",
    "# Call the function with the example hits and query length\n",
    "result = find_most_likely_broken_rules(hits, query_length)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_index': 'regex_rules_index', '_id': 'ge2TDpIBUlWb17Z57eb8', '_score': 7.5438104, '_source': {'sanitization_term': 'forecast', 'entity': 'forward_looking_statements'}}, {'_index': 'regex_rules_index', '_id': 'k-2TDpIBUlWb17Z57eb8', '_score': 7.4330792, '_source': {'sanitization_term': 'forecasted price', 'entity': 'price_target_inquiry'}}]\n",
      "Total hits: 2\n",
      "Term matched: forecast, Entity: forward_looking_statements, Score: 7.5438104\n",
      "Term matched: forecasted price, Entity: price_target_inquiry, Score: 7.4330792\n"
     ]
    }
   ],
   "source": [
    "text = \"what is the weather forecast?\"\n",
    "\n",
    "# Define the query\n",
    "query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                    \"query\": text,\n",
    "                    \"fields\": [\"sanitization_term\"],\n",
    "                    \"operator\": \"or\",\n",
    "                    \"fuzziness\": \"AUTO\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Execute the query\n",
    "response = await es.search(index=\"regex_rules_index\", body=query)\n",
    "\n",
    "# Extract hits from the response\n",
    "hits = response['hits']['hits']\n",
    "\n",
    "# Define the function (from the provided code)\n",
    "def find_most_likely_broken_rules(hits, query_length, base_threshold=1.0, query_length_factor=0.01, tolerance_factor=0.05, epsilon=0.0001, top_n=10):\n",
    "    broken_rules = []\n",
    "    counter = 0\n",
    "\n",
    "    if not hits:\n",
    "        return {\"message\": \"No matches found.\"}  # No hits found\n",
    "\n",
    "    # Calculate dynamic min score threshold based on query length\n",
    "    min_score_threshold = base_threshold - (query_length * query_length_factor)\n",
    "\n",
    "    # Process only top N results for performance reasons\n",
    "    limited_hits = hits[:top_n]\n",
    "\n",
    "    # Initialize the previous score with the first hit if it meets the threshold\n",
    "    first_hit = limited_hits[0]\n",
    "    previous_score = first_hit['_score']\n",
    "\n",
    "    if previous_score >= min_score_threshold:\n",
    "        broken_rules.append(first_hit)\n",
    "        counter += 1\n",
    "\n",
    "    # Iterate over remaining results\n",
    "    for hit in limited_hits[1:]:\n",
    "        current_score = hit['_score']\n",
    "\n",
    "        # Check if score meets the dynamically calculated minimum threshold\n",
    "        if current_score >= min_score_threshold:\n",
    "\n",
    "            # Calculate the adaptive score threshold as a percentage of the previous score\n",
    "            adaptive_threshold = previous_score * tolerance_factor\n",
    "\n",
    "            # Check if the difference between the current and previous score is within the adaptive threshold\n",
    "            if abs(current_score - previous_score) <= adaptive_threshold or abs(current_score - previous_score) < epsilon:\n",
    "\n",
    "                # Add the result to the broken rules list\n",
    "                broken_rules.append(hit)\n",
    "                counter += 1\n",
    "                previous_score = current_score  # Update previous score\n",
    "\n",
    "    # If no results meet the threshold, provide a failover\n",
    "    if counter == 0:\n",
    "        return {\"message\": \"No likely broken rules found with current thresholds. Consider broadening criteria.\"}\n",
    "\n",
    "    return broken_rules\n",
    "\n",
    "# Example query length (number of words in the query)\n",
    "query_length = len(text.split())\n",
    "\n",
    "# Call the function with the extracted hits and query length\n",
    "result = find_most_likely_broken_rules(hits, query_length)\n",
    "print(result)\n",
    "if result:\n",
    "    print(f\"Total hits: {len(result)}\")\n",
    "    for hit in result:\n",
    "        print(f\"Term matched: {hit['_source']['sanitization_term']}, Entity: {hit['_source']['entity']}, Score: {hit['_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPACY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 252\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompliant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m: topic}\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m automaton \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_aho_corasick_automaton_from_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOMPLIANCE_RULES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the forecasted price of Tesla\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms stock in the next quarter?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m result \u001b[38;5;241m=\u001b[39m check_compliance(query, automaton)\n",
      "Cell \u001b[0;32mIn[152], line 17\u001b[0m, in \u001b[0;36mbuild_aho_corasick_automaton_from_labels\u001b[0;34m(labeled_data)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_aho_corasick_automaton_from_labels\u001b[39m(labeled_data):\n\u001b[1;32m     15\u001b[0m     automaton \u001b[38;5;241m=\u001b[39m ahocorasick\u001b[38;5;241m.\u001b[39mAutomaton()\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m query, topic \u001b[38;5;129;01min\u001b[39;00m labeled_data:\n\u001b[1;32m     18\u001b[0m         query \u001b[38;5;241m=\u001b[39m preprocess_query(query)  \u001b[38;5;66;03m# Preprocess query\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# Tokenize the query\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import ahocorasick\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_query(query):\n",
    "    # Lowercase the query and remove punctuation\n",
    "    query = query.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return query\n",
    "\n",
    "# Build Aho-Corasick Automaton for Compliance Rules\n",
    "def build_aho_corasick_automaton_from_labels(labeled_data):\n",
    "    automaton = ahocorasick.Automaton()\n",
    "\n",
    "    for query, topic in labeled_data:\n",
    "        query = preprocess_query(query)  # Preprocess query\n",
    "        tokens = query.split()  # Tokenize the query\n",
    "\n",
    "        # Add each token to the automaton with the associated topic (label)\n",
    "        for token in tokens:\n",
    "            automaton.add_word(token, (topic, token))\n",
    "\n",
    "    automaton.make_automaton()  # Finalize the automaton\n",
    "    return automaton\n",
    "\n",
    "# Function to detect non-compliance using the Aho-Corasick automaton\n",
    "def detect_non_compliance_aho_corasick(automaton, query):\n",
    "    detected_rules = {}\n",
    "    query_lower = preprocess_query(query)\n",
    "\n",
    "    for end_index, (rule, term) in automaton.iter(query_lower):\n",
    "        if rule not in detected_rules:\n",
    "            detected_rules[rule] = []\n",
    "        detected_rules[rule].append(term)\n",
    "\n",
    "    return detected_rules if detected_rules else None\n",
    "\n",
    "# Function to generate dataset with random queries and corresponding labels\n",
    "def generate_data_with_labels(templates, num_samples=100):\n",
    "    queries_and_labels = []\n",
    "    for _ in range(num_samples):\n",
    "        topic = random.choice(list(templates.keys()))  # Select topic as label\n",
    "        template = random.choice(templates[topic])  # Select query template\n",
    "        query = template.format(\n",
    "            company=random.choice([\"Tesla\", \"Apple\", \"Amazon\", \"Google\", \"Microsoft\", \"Goldman Sachs\", \"Pfizer\", \"JP Morgan\", \"ExxonMobil\", \"Chevron\", \"Meta\", \"Coca-Cola\"]),\n",
    "            time_period=random.choice([\"next quarter\", \"next fiscal year\", \"this month\", \"the next 5 years\", \"q1\", \"q2\", \"q3\", \"q4\"]),\n",
    "            sector=random.choice([\"automotive\", \"tech\", \"financial\", \"energy\", \"pharmaceutical\", \"consumer goods\"]),\n",
    "            market_conditions=random.choice([\"rising inflation\", \"global instability\", \"market volatility\"]),\n",
    "            event=random.choice([\"merger\", \"acquisition\", \"earnings report\", \"restructuring\"]),\n",
    "            document=random.choice([\"report\", \"contract\", \"whitepaper\", \"presentation\"]),\n",
    "            action=random.choice([\"skyrocket\", \"double\", \"decrease\", \"triple\"]),\n",
    "            price=random.choice([\"$1000\", \"$1500\", \"2000\", \"fifty\"]),\n",
    "            location=random.choice([\"New York\", \"London\", \"Tokyo\", \"Paris\"]),\n",
    "            country=random.choice([\"the United States\", \"France\", \"Germany\", \"Japan\"]),\n",
    "            years=random.randint(1, 20),\n",
    "            information_type=random.choice([\"confidential\", \"insider\", \"non-public\"]),\n",
    "            regulatory_body=random.choice([\"SEC\", \"FTC\", \"FCA\", \"EU Commission\"]),\n",
    "            confidentiality=random.choice([\"confidential\", \"non-public\", \"privileged\"]),\n",
    "            insider=random.choice([\"insider\", \"internal\", \"secret\"]),\n",
    "            merger_acquisition=random.choice([\"merger\", \"acquisition\"]),\n",
    "            contracts=random.choice([\"contracts\", \"agreements\"]),\n",
    "            no_risk=random.choice([\"no risk\", \"little risk\"]),\n",
    "            risk_free=random.choice([\"risk-free\", \"guaranteed\"]),\n",
    "            high_return=random.choice([\"high returns\", \"massive gains\"]),\n",
    "            promise=random.choice([\"promise\", \"guarantee\"]),\n",
    "            gains=random.choice([\"massive gains\", \"great returns\"]),\n",
    "            no_downside=random.choice([\"no downside\", \"no risk\"])\n",
    "        )\n",
    "        # Append query and its corresponding topic as label\n",
    "        queries_and_labels.append((query, topic))\n",
    "    return queries_and_labels\n",
    "\n",
    "# Comprehensive Compliance Rules Dictionary with Enhanced Templates (as defined before)\n",
    "COMPLIANCE_RULES = {\n",
    "    \"forward_looking_statements\": [\n",
    "        \"What are {company}'s revenue projections for the {time_period}?\",\n",
    "        \"Will {company}'s stock {action} based on {market_conditions} over the {time_period}?\",\n",
    "        \"Do you expect {company}'s {sector} performance to outpace competitors in {location} over the {time_period}?\",\n",
    "        \"How will {company}'s strategic initiatives impact their earnings event in the {time_period}?\",\n",
    "        \"Is {company} likely to adjust its capital allocation strategy given {market_conditions}?\"\n",
    "    ],\n",
    "    \"insider_information\": [\n",
    "        \"Is there any {information_type} information about {company}'s upcoming {event}?\",\n",
    "        \"Can you share any {insider} insights on {company}'s earnings event for {time_period}?\",\n",
    "        \"What {confidentiality} details can you provide about {company}'s {document} and its impact on their stock price?\",\n",
    "        \"Is there any {confidentiality} information about {company}'s {merger_acquisition} announcement?\",\n",
    "        \"Do you have access to any {insider} reports about {company}'s {contracts} or agreements?\"\n",
    "    ],\n",
    "    \"price_target_inquiry\": [\n",
    "        \"What's your price target for {company}'s stock over {time_period}?\",\n",
    "        \"Do you see {company}'s stock reaching {price} given {market_conditions}?\",\n",
    "        \"What are the expected {sector} returns for {company} by {time_period}?\",\n",
    "        \"Could {company}'s stock hit {price} in the {time_period}?\",\n",
    "        \"What valuation do you anticipate for {company} based on {market_conditions} and {sector} trends?\"\n",
    "    ],\n",
    "    \"mnpi\": [\n",
    "        \"Is there any {insider} data on {company}'s earnings event before its public release?\",\n",
    "        \"Do you have {confidentiality} info on {company}'s {merger_acquisition}?\",\n",
    "        \"Can you provide any {information_type} insights on {company}'s project?\",\n",
    "        \"Are there any material non-public details regarding {company}'s earnings event?\",\n",
    "        \"Can you disclose {company}'s {document} under {insider} access?\"\n",
    "    ],\n",
    "    \"misleading_statements\": [\n",
    "        \"{company}'s stock is guaranteed to {action} over the {time_period}.\",\n",
    "        \"This trade has {no_risk}; {company} is a sure bet.\",\n",
    "        \"I {promise} {company}'s stock will hit {price} in {time_period}.\",\n",
    "        \"There's {no_downside} to investing in {company} right now.\",\n",
    "        \"Investing in {company} is a {risk_free} opportunity.\"\n",
    "    ],\n",
    "    \"over_promising\": [\n",
    "        \"Expect {high_return} from {company} in the {time_period}.\",\n",
    "        \"{company} is set to {action} over the next {time_period}.\",\n",
    "        \"You can't lose with {company}; the stock is poised to {action}.\",\n",
    "        \"{company} will {action} in the {time_period}—the market is aligned perfectly.\",\n",
    "        \"This is the perfect moment to get in on {company}'s {event} before they {action}.\"\n",
    "    ],\n",
    "    \"insider_trading_signals\": [\n",
    "        \"Are we in a blackout period for {company}'s stock?\",\n",
    "        \"Is it permissible to trade {company}'s stock during this quiet period?\",\n",
    "        \"When does the blackout period end for {company}?\",\n",
    "        \"Can we execute trades on {company} now, or are we restricted?\",\n",
    "        \"Is the trading window currently closed for {company}?\"\n",
    "    ],\n",
    "    \"nda_violations\": [\n",
    "        \"Are we allowed to disclose {company}'s {document} under the NDA?\",\n",
    "        \"Does the NDA cover {company}'s project, or can we discuss it?\",\n",
    "        \"Is this {document} related to {company} covered under the NDA?\",\n",
    "        \"Are there restrictions on sharing {company}'s {contracts} with external parties?\",\n",
    "        \"Can we talk about {company}'s project, or is it protected by NDA?\"\n",
    "    ],\n",
    "    \"conflict_of_interest\": [\n",
    "        \"Do you have any personal interest in this {merger_acquisition} involving {company}?\",\n",
    "        \"Is there any conflict of interest with {company}'s deal that needs disclosure?\",\n",
    "        \"Does {company}'s project pose any potential conflict of interest?\",\n",
    "        \"Are there any undisclosed relationships that could affect {company}'s transaction?\",\n",
    "        \"Is there a risk of bias in {company}'s {event} due to a conflict of interest?\"\n",
    "    ],\n",
    "    \"fiduciary_breach\": [\n",
    "        \"Has {company}'s trustee breached their fiduciary duty?\",\n",
    "        \"Is {company} involved in a breach of fiduciary trust with this project?\",\n",
    "        \"Could {company} face legal action for fiduciary mismanagement?\",\n",
    "        \"Is there evidence that {company} has violated fiduciary obligations?\",\n",
    "        \"Do you believe {company}'s actions are a breach of fiduciary duty?\"\n",
    "    ],\n",
    "    \"high_risk_language\": [\n",
    "        \"{company}'s stock is highly volatile but could provide {high_return}.\",\n",
    "        \"If you're willing to take a chance, {company}'s stock could {action}.\",\n",
    "        \"{company} is a high-risk, high-reward play given {market_conditions}.\",\n",
    "        \"This is a speculative investment on {company}, but the upside is massive.\",\n",
    "        \"Investing in {company} comes with high risk language, but the potential rewards are great.\"\n",
    "    ],\n",
    "    \"unauthorized_disclosures\": [\n",
    "        \"Are we authorized to share {company}'s {document} externally?\",\n",
    "        \"Can we disclose {company}'s {document}, or is it restricted?\",\n",
    "        \"Is {company}'s {event} considered confidential, or can we discuss it?\",\n",
    "        \"Are we allowed to distribute {company}'s {document} before it's official?\",\n",
    "        \"Is {company}'s {document} ready for public release, or is it still restricted?\"\n",
    "    ],\n",
    "    \"investment_advice_without_disclaimer\": [\n",
    "        \"I strongly advise buying {company}'s stock now.\",\n",
    "        \"You should invest in {company}'s stock before the {event} is announced.\",\n",
    "        \"Sell {company}'s stock before {market_conditions} change.\",\n",
    "        \"Increase your position in {company}'s stock; {high_return} is expected.\",\n",
    "        \"This is a {risk_free} opportunity to invest in {company}.\"\n",
    "    ],\n",
    "    \"regulatory_speculation\": [\n",
    "        \"Will {company} face regulatory penalties from the {regulatory_body}?\",\n",
    "        \"Do you expect {company} to be fined by the {regulatory_body} due to activity?\",\n",
    "        \"Could the {regulatory_body} launch an investigation into {company}?\",\n",
    "        \"Is there a chance {company}'s actions could result in sanctions by the {regulatory_body}?\",\n",
    "        \"Do you think {company} will face enforcement action from the {regulatory_body}?\"\n",
    "    ],\n",
    "    \"legal_violations\": [\n",
    "        \"Is {company}'s activity illegal under current regulations?\",\n",
    "        \"Could {company} be involved in legal violations related to their project?\",\n",
    "        \"Do you think {company}'s {action} could be considered unlawful?\",\n",
    "        \"Could {company} face litigation due to activity in the {sector}?\",\n",
    "        \"Is there any risk of legal action against {company} for activity?\"\n",
    "    ],\n",
    "    \"market_manipulation\": [\n",
    "        \"Is {company} involved in any market manipulation tactics like spoofing?\",\n",
    "        \"Do you think the recent activity in {company}'s stock is a pump-and-dump?\",\n",
    "        \"Is {company} engaging in price manipulation, or is this just market movement?\",\n",
    "        \"Could {company}'s actions be considered wash trading or market manipulation?\",\n",
    "        \"Is there concern that {company} is influencing the market in the {sector}?\"\n",
    "    ],\n",
    "    \"unapproved_marketing\": [\n",
    "        \"Has {company}'s marketing campaign been approved for release?\",\n",
    "        \"Are we allowed to distribute {company}'s advertisement, or is it unapproved?\",\n",
    "        \"Is {company}'s marketing material ready for public view?\",\n",
    "        \"Can we release {company}'s marketing campaign, or does it need more vetting?\",\n",
    "        \"Is this an unapproved draft of {company}'s campaign?\"\n",
    "    ],\n",
    "    \"weather\": [\n",
    "        \"How will weather in {location} affect {company}'s {event}?\",\n",
    "        \"Does {company}'s stock depend on weather patterns in {location}?\",\n",
    "        \"Will {location}'s hurricane season impact {company}'s {sector} performance?\",\n",
    "        \"How does extreme weather in {country} affect {company}'s supply chain?\",\n",
    "        \"Will weather changes in {country} affect {company}'s stock?\"\n",
    "    ],\n",
    "    \"general\": [\n",
    "        \"What are {company}'s key products?\",\n",
    "        \"How does {company}'s leadership affect stock performance?\",\n",
    "        \"How has {company}'s growth strategy evolved over the past {years} years?\",\n",
    "        \"What is {company}'s position in the {sector} market?\",\n",
    "        \"What impact will {company}'s new product launch have on the {sector} market?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate a dataset of queries with labels\n",
    "training_queries_with_labels = generate_data_with_labels(COMPLIANCE_RULES, num_samples=1000)\n",
    "\n",
    "# Split the data into training queries and corresponding labels\n",
    "training_queries = [query for query, label in training_queries_with_labels]\n",
    "labels = [label for query, label in training_queries_with_labels]\n",
    "\n",
    "# Vectorize the training queries\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(training_queries)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression().fit(X_train, labels)\n",
    "\n",
    "# Classify a new query\n",
    "def classify_topic(query):\n",
    "    X_query = vectorizer.transform([query])\n",
    "    predicted_topic = clf.predict(X_query)\n",
    "    return predicted_topic[0]\n",
    "\n",
    "# Check compliance using both topic classification and Aho-Corasick\n",
    "def check_compliance(query, automaton, min_topic_confidence=0.8):\n",
    "    # Preprocess query\n",
    "    query = preprocess_query(query)\n",
    "\n",
    "    # Step 1: Use Aho-Corasick to detect compliance issues\n",
    "    compliance_issues = detect_non_compliance_aho_corasick(automaton, query)\n",
    "\n",
    "    # Step 2: Classify topic (e.g., finance, general, weather)\n",
    "    topic = classify_topic(query)\n",
    "\n",
    "    c_label = ['weather', 'general']\n",
    "    # If the topic is finance/business and we have compliance issues\n",
    "    if topic not in c_label and compliance_issues:\n",
    "        return {\"status\": \"Non-Compliant\", \"rules\": compliance_issues, \"topic\": topic}\n",
    "    \n",
    "    # Else, return as compliant\n",
    "    return {\"status\": \"Compliant\", \"topic\": topic}\n",
    "\n",
    "# Example usage\n",
    "automaton = build_aho_corasick_automaton_from_labels(COMPLIANCE_RULES)\n",
    "query = \"What is the forecasted price of Tesla's stock in the next quarter?\"\n",
    "result = check_compliance(query, automaton)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Are there restrictions on sharing Microsoft's contracts with external parties?\", 'nda_violations'), (\"Does Tesla's stock depend on weather patterns in Paris?\", 'weather'), (\"Are we allowed to disclose Pfizer's presentation under the NDA?\", 'nda_violations'), ('Could JP Morgan be involved in legal violations related to their project?', 'legal_violations'), ('Could Amazon be involved in legal violations related to their project?', 'legal_violations'), (\"Can we disclose JP Morgan's contract, or is it restricted?\", 'unauthorized_disclosures'), (\"Is this an unapproved draft of Tesla's campaign?\", 'unapproved_marketing'), ('This is a speculative investment on ExxonMobil, but the upside is massive.', 'high_risk_language'), ('Is this report related to Amazon covered under the NDA?', 'nda_violations'), ('Can we execute trades on Meta now, or are we restricted?', 'insider_trading_signals')]\n",
      "{'status': 'Non-Compliant', 'non_compliant_sentences': [\"what's novonordiks price?\"], 'topic': np.str_('nda_violations')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/moozimoozi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import ahocorasick\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "# Function to preprocess queries (lowercase and remove punctuation)\n",
    "def preprocess_query(query):\n",
    "    query = query.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return query\n",
    "\n",
    "def check_sentence_compliance(sentence, automaton):\n",
    "    # Preprocess sentence\n",
    "    sentence = preprocess_query(sentence)\n",
    "    \n",
    "    # Detect compliance issues using Aho-Corasick\n",
    "    compliance_issues = detect_non_compliance_aho_corasick(automaton, sentence)\n",
    "    \n",
    "    # Return True if compliant, otherwise False\n",
    "    return compliance_issues is None\n",
    "\n",
    "# Function to build the Aho-Corasick automaton using labeled data\n",
    "def build_aho_corasick_automaton_from_labels(labeled_data):\n",
    "    automaton = ahocorasick.Automaton()\n",
    "\n",
    "    for query, topic in labeled_data:\n",
    "        query = preprocess_query(query)  # Preprocess query\n",
    "        tokens = query.split()  # Tokenize the query\n",
    "\n",
    "        # Add each token to the automaton with the associated topic (label)\n",
    "        for token in tokens:\n",
    "            automaton.add_word(token, (topic, token))\n",
    "\n",
    "    automaton.make_automaton()  # Finalize the automaton\n",
    "    return automaton\n",
    "\n",
    "# Function to detect non-compliance using the Aho-Corasick automaton\n",
    "def detect_non_compliance_aho_corasick(automaton, query):\n",
    "    detected_rules = {}\n",
    "    query_lower = preprocess_query(query)\n",
    "\n",
    "    for end_index, (rule, term) in automaton.iter(query_lower):\n",
    "        if rule not in detected_rules:\n",
    "            detected_rules[rule] = []\n",
    "        detected_rules[rule].append(term)\n",
    "\n",
    "    return detected_rules if detected_rules else None\n",
    "\n",
    "# Function to generate labeled data (queries and labels)\n",
    "def generate_data_with_labels(templates, num_samples=100):\n",
    "    queries_and_labels = set()  # Use a set to store unique (query, topic) pairs\n",
    "    while len(queries_and_labels) < num_samples:\n",
    "        topic = random.choice(list(templates.keys()))  # Select topic as label\n",
    "        template = random.choice(templates[topic])  # Select query template\n",
    "        query = template.format(\n",
    "            company=random.choice([\"Tesla\", \"Apple\", \"Amazon\", \"Google\", \"Microsoft\", \"Goldman Sachs\", \"Pfizer\", \"JP Morgan\", \"ExxonMobil\", \"Chevron\", \"Meta\", \"Coca-Cola\"]),\n",
    "            time_period=random.choice([\"next quarter\", \"next fiscal year\", \"this month\", \"the next 5 years\"]),\n",
    "            sector=random.choice([\"automotive\", \"tech\", \"financial\", \"energy\", \"pharmaceutical\", \"consumer goods\"]),\n",
    "            market_conditions=random.choice([\"rising inflation\", \"global instability\", \"market volatility\"]),\n",
    "            event=random.choice([\"merger\", \"acquisition\", \"earnings report\", \"restructuring\"]),\n",
    "            document=random.choice([\"report\", \"contract\", \"whitepaper\", \"presentation\"]),\n",
    "            action=random.choice([\"skyrocket\", \"double\", \"decrease\", \"triple\"]),\n",
    "            price=random.choice([\"$1000\", \"$1500\", \"2000\", \"fifty\"]),\n",
    "            location=random.choice([\"New York\", \"London\", \"Tokyo\", \"Paris\"]),\n",
    "            country=random.choice([\"the United States\", \"France\", \"Germany\", \"Japan\"]),\n",
    "            years=random.randint(1, 20),\n",
    "            information_type=random.choice([\"confidential\", \"insider\", \"non-public\"]),\n",
    "            regulatory_body=random.choice([\"SEC\", \"FTC\", \"FCA\", \"EU Commission\"]),\n",
    "            confidentiality=random.choice([\"confidential\", \"non-public\", \"privileged\"]),\n",
    "            insider=random.choice([\"insider\", \"internal\", \"secret\"]),\n",
    "            merger_acquisition=random.choice([\"merger\", \"acquisition\"]),\n",
    "            contracts=random.choice([\"contracts\", \"agreements\"]),\n",
    "            no_risk=random.choice([\"no risk\", \"little risk\"]),\n",
    "            risk_free=random.choice([\"risk-free\", \"guaranteed\"]),\n",
    "            high_return=random.choice([\"high returns\", \"massive gains\"]),\n",
    "            promise=random.choice([\"promise\", \"guarantee\"]),\n",
    "            gains=random.choice([\"massive gains\", \"great returns\"]),\n",
    "            no_downside=random.choice([\"no downside\", \"no risk\"])\n",
    "        )\n",
    "\n",
    "        # Add query and its corresponding topic as a tuple to the set\n",
    "        queries_and_labels.add((query, topic))\n",
    "\n",
    "    # Return the unique set of queries as a list\n",
    "    return list(queries_and_labels)\n",
    "\n",
    "# Example templates for generating data\n",
    "COMPLIANCE_RULES = {\n",
    "    \"forward_looking_statements\": [\n",
    "        \"What are {company}'s revenue projections for the {time_period}?\",\n",
    "        \"Will {company}'s stock {action} based on {market_conditions} over the {time_period}?\",\n",
    "        \"Do you expect {company}'s {sector} performance to outpace competitors in {location} over the {time_period}?\",\n",
    "        \"How will {company}'s strategic initiatives impact their earnings event in the {time_period}?\",\n",
    "        \"Is {company} likely to adjust its capital allocation strategy given {market_conditions}?\"\n",
    "    ],\n",
    "    \"insider_information\": [\n",
    "        \"Is there any {information_type} information about {company}'s upcoming {event}?\",\n",
    "        \"Can you share any {insider} insights on {company}'s earnings event for {time_period}?\",\n",
    "        \"What {confidentiality} details can you provide about {company}'s {document} and its impact on their stock price?\",\n",
    "        \"Is there any {confidentiality} information about {company}'s {merger_acquisition} announcement?\",\n",
    "        \"Do you have access to any {insider} reports about {company}'s {contracts} or agreements?\"\n",
    "    ],\n",
    "    \"price_target_inquiry\": [\n",
    "        \"What's your price target for {company}'s stock over {time_period}?\",\n",
    "        \"Do you see {company}'s stock reaching {price} given {market_conditions}?\",\n",
    "        \"What are the expected {sector} returns for {company} by {time_period}?\",\n",
    "        \"Could {company}'s stock hit {price} in the {time_period}?\",\n",
    "        \"What valuation do you anticipate for {company} based on {market_conditions} and {sector} trends?\"\n",
    "    ],\n",
    "    \"mnpi\": [\n",
    "        \"Is there any {insider} data on {company}'s earnings event before its public release?\",\n",
    "        \"Do you have {confidentiality} info on {company}'s {merger_acquisition}?\",\n",
    "        \"Can you provide any {information_type} insights on {company}'s project?\",\n",
    "        \"Are there any material non-public details regarding {company}'s earnings event?\",\n",
    "        \"Can you disclose {company}'s {document} under {insider} access?\"\n",
    "    ],\n",
    "    \"misleading_statements\": [\n",
    "        \"{company}'s stock is guaranteed to {action} over the {time_period}.\",\n",
    "        \"This trade has {no_risk}; {company} is a sure bet.\",\n",
    "        \"I {promise} {company}'s stock will hit {price} in {time_period}.\",\n",
    "        \"There's {no_downside} to investing in {company} right now.\",\n",
    "        \"Investing in {company} is a {risk_free} opportunity.\"\n",
    "    ],\n",
    "    \"over_promising\": [\n",
    "        \"Expect {high_return} from {company} in the {time_period}.\",\n",
    "        \"{company} is set to {action} over the next {time_period}.\",\n",
    "        \"You can't lose with {company}; the stock is poised to {action}.\",\n",
    "        \"{company} will {action} in the {time_period}—the market is aligned perfectly.\",\n",
    "        \"This is the perfect moment to get in on {company}'s {event} before they {action}.\"\n",
    "    ],\n",
    "    \"insider_trading_signals\": [\n",
    "        \"Are we in a blackout period for {company}'s stock?\",\n",
    "        \"Is it permissible to trade {company}'s stock during this quiet period?\",\n",
    "        \"When does the blackout period end for {company}?\",\n",
    "        \"Can we execute trades on {company} now, or are we restricted?\",\n",
    "        \"Is the trading window currently closed for {company}?\"\n",
    "    ],\n",
    "    \"nda_violations\": [\n",
    "        \"Are we allowed to disclose {company}'s {document} under the NDA?\",\n",
    "        \"Does the NDA cover {company}'s project, or can we discuss it?\",\n",
    "        \"Is this {document} related to {company} covered under the NDA?\",\n",
    "        \"Are there restrictions on sharing {company}'s {contracts} with external parties?\",\n",
    "        \"Can we talk about {company}'s project, or is it protected by NDA?\"\n",
    "    ],\n",
    "    \"conflict_of_interest\": [\n",
    "        \"Do you have any personal interest in this {merger_acquisition} involving {company}?\",\n",
    "        \"Is there any conflict of interest with {company}'s deal that needs disclosure?\",\n",
    "        \"Does {company}'s project pose any potential conflict of interest?\",\n",
    "        \"Are there any undisclosed relationships that could affect {company}'s transaction?\",\n",
    "        \"Is there a risk of bias in {company}'s {event} due to a conflict of interest?\"\n",
    "    ],\n",
    "    \"fiduciary_breach\": [\n",
    "        \"Has {company}'s trustee breached their fiduciary duty?\",\n",
    "        \"Is {company} involved in a breach of fiduciary trust with this project?\",\n",
    "        \"Could {company} face legal action for fiduciary mismanagement?\",\n",
    "        \"Is there evidence that {company} has violated fiduciary obligations?\",\n",
    "        \"Do you believe {company}'s actions are a breach of fiduciary duty?\"\n",
    "    ],\n",
    "    \"high_risk_language\": [\n",
    "        \"{company}'s stock is highly volatile but could provide {high_return}.\",\n",
    "        \"If you're willing to take a chance, {company}'s stock could {action}.\",\n",
    "        \"{company} is a high-risk, high-reward play given {market_conditions}.\",\n",
    "        \"This is a speculative investment on {company}, but the upside is massive.\",\n",
    "        \"Investing in {company} comes with high risk language, but the potential rewards are great.\"\n",
    "    ],\n",
    "    \"unauthorized_disclosures\": [\n",
    "        \"Are we authorized to share {company}'s {document} externally?\",\n",
    "        \"Can we disclose {company}'s {document}, or is it restricted?\",\n",
    "        \"Is {company}'s {event} considered confidential, or can we discuss it?\",\n",
    "        \"Are we allowed to distribute {company}'s {document} before it's official?\",\n",
    "        \"Is {company}'s {document} ready for public release, or is it still restricted?\"\n",
    "    ],\n",
    "    \"investment_advice_without_disclaimer\": [\n",
    "        \"I strongly advise buying {company}'s stock now.\",\n",
    "        \"You should invest in {company}'s stock before the {event} is announced.\",\n",
    "        \"Sell {company}'s stock before {market_conditions} change.\",\n",
    "        \"Increase your position in {company}'s stock; {high_return} is expected.\",\n",
    "        \"This is a {risk_free} opportunity to invest in {company}.\"\n",
    "    ],\n",
    "    \"regulatory_speculation\": [\n",
    "        \"Will {company} face regulatory penalties from the {regulatory_body}?\",\n",
    "        \"Do you expect {company} to be fined by the {regulatory_body} due to activity?\",\n",
    "        \"Could the {regulatory_body} launch an investigation into {company}?\",\n",
    "        \"Is there a chance {company}'s actions could result in sanctions by the {regulatory_body}?\",\n",
    "        \"Do you think {company} will face enforcement action from the {regulatory_body}?\"\n",
    "    ],\n",
    "    \"legal_violations\": [\n",
    "        \"Is {company}'s activity illegal under current regulations?\",\n",
    "        \"Could {company} be involved in legal violations related to their project?\",\n",
    "        \"Do you think {company}'s {action} could be considered unlawful?\",\n",
    "        \"Could {company} face litigation due to activity in the {sector}?\",\n",
    "        \"Is there any risk of legal action against {company} for activity?\"\n",
    "    ],\n",
    "    \"market_manipulation\": [\n",
    "        \"Is {company} involved in any market manipulation tactics like spoofing?\",\n",
    "        \"Do you think the recent activity in {company}'s stock is a pump-and-dump?\",\n",
    "        \"Is {company} engaging in price manipulation, or is this just market movement?\",\n",
    "        \"Could {company}'s actions be considered wash trading or market manipulation?\",\n",
    "        \"Is there concern that {company} is influencing the market in the {sector}?\"\n",
    "    ],\n",
    "    \"unapproved_marketing\": [\n",
    "        \"Has {company}'s marketing campaign been approved for release?\",\n",
    "        \"Are we allowed to distribute {company}'s advertisement, or is it unapproved?\",\n",
    "        \"Is {company}'s marketing material ready for public view?\",\n",
    "        \"Can we release {company}'s marketing campaign, or does it need more vetting?\",\n",
    "        \"Is this an unapproved draft of {company}'s campaign?\"\n",
    "    ],\n",
    "    \"weather\": [\n",
    "        \"How will weather in {location} affect {company}'s {event}?\",\n",
    "        \"Does {company}'s stock depend on weather patterns in {location}?\",\n",
    "        \"Will {location}'s hurricane season impact {company}'s {sector} performance?\",\n",
    "        \"How does extreme weather in {country} affect {company}'s supply chain?\",\n",
    "        \"Will weather changes in {country} affect {company}'s stock?\"\n",
    "    ],\n",
    "    \"general\": [\n",
    "        \"What are {company}'s key products?\",\n",
    "        \"How does {company}'s leadership affect stock performance?\",\n",
    "        \"How has {company}'s growth strategy evolved over the past {years} years?\",\n",
    "        \"What is {company}'s position in the {sector} market?\",\n",
    "        \"What impact will {company}'s new product launch have on the {sector} market?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate a dataset of 1000 queries with compliance rule labels\n",
    "training_queries_with_labels = generate_data_with_labels(COMPLIANCE_RULES, num_samples=10)\n",
    "print(training_queries_with_labels)\n",
    "# Build the Aho-Corasick automaton using the generated data\n",
    "automaton = build_aho_corasick_automaton_from_labels(training_queries_with_labels)\n",
    "\n",
    "# Function to classify a query's topic using a trained model (logistic regression)\n",
    "def classify_topic(query, clf, vectorizer):\n",
    "    X_query = vectorizer.transform([query])\n",
    "    predicted_topic = clf.predict(X_query)\n",
    "    return predicted_topic[0]\n",
    "\n",
    "# Train a simple logistic regression model for topic classification\n",
    "def train_topic_classifier(queries_with_labels):\n",
    "    queries = [item[0] for item in queries_with_labels]\n",
    "    labels = [item[1] for item in queries_with_labels]\n",
    "\n",
    "    # Vectorize the queries\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(queries)\n",
    "\n",
    "    # Train the logistic regression classifier\n",
    "    clf = LogisticRegression().fit(X_train, labels)\n",
    "    return clf, vectorizer\n",
    "\n",
    "# Train the classifier\n",
    "clf, vectorizer = train_topic_classifier(training_queries_with_labels)\n",
    "\n",
    "# Comprehensive compliance check function\n",
    "def check_compliance(query, automaton, clf, vectorizer):\n",
    "    sentences = sent_tokenize(query)\n",
    "\n",
    "    # Step 1: Use Aho-Corasick to detect compliance issues\n",
    "    # compliance_issues = detect_non_compliance_aho_corasick(automaton, query)\n",
    "\n",
    "    non_compliant_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if not check_sentence_compliance(sentence, automaton):\n",
    "            non_compliant_sentences.append(sentence)\n",
    "\n",
    "    # Step 2: Classify the topic using the logistic regression classifier\n",
    "    topic = classify_topic(query, clf, vectorizer)\n",
    "\n",
    "    c_label = ['weather', 'general']\n",
    "    # If the topic is finance/business and we have compliance issues\n",
    "    # if topic not in c_label and compliance_issues:\n",
    "    #     return {\"status\": \"Non-Compliant\", \"rules\": compliance_issues, \"topic\": topic}\n",
    "    \n",
    "    # return {\"status\": \"Compliant\", \"topic\": topic}\n",
    "\n",
    "    # Determine overall compliance\n",
    "    if topic not in c_label and non_compliant_sentences:\n",
    "        return {\n",
    "            \"status\": \"Non-Compliant\",\n",
    "            \"non_compliant_sentences\": non_compliant_sentences,\n",
    "            \"topic\": topic\n",
    "        }\n",
    "        \n",
    "    return {\"status\": \"Compliant\", \"topic\": topic}\n",
    "\n",
    "# Example query to check compliance\n",
    "query = \"what's novonordiks price?\"\n",
    "result = check_compliance(query, automaton, clf, vectorizer)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'Non-Compliant', 'non_compliant_sentences': ['In this example, we will consider a dictionary consisting of the following words: {a, ab, bab, bc, bca, c, caa}.', 'The graph below is the Aho Corasick data structure constructed from the specified dictionary, with each row in the table representing a node in the trie, with the column path indicating the (unique) sequence of characters from the root to the node.', 'The data structure has one node for every prefix of every string in the dictionary.', 'So if (bca) is in the dictionary, then there will be nodes for (bca), (bc), (b), and ().', 'If a node is in the dictionary then it is a blue node.', 'Otherwise it is a grey node.', \"There is a black directed 'child' arc from each node to a node whose name is found by appending one character.\", 'So there is a black arc from (bc) to (bca).', \"There is a blue directed 'suffix' arc from each node to the node that is the longest possible strict suffix of it in the graph.\", 'For example, for node (caa), its strict suffixes are (aa) and (a) and ().', 'The longest of these that exists in the graph is (a).', 'So there is a blue arc from (caa) to (a).', 'The blue arcs can be computed in linear time by performing a breadth-first search [potential suffix node will always be at lower level] starting from the root.', \"The target for the blue arc of a visited node can be found by following its parent's blue arc to its longest suffix node and searching for a child of the suffix node whose character matches that of the visited node.\", 'If the character does not exist as a child, we can find the next longest suffix (following the blue arc again) and then search for the character.', 'We can do this until we either find the character (as child of a node) or we reach the root (which will always be a suffix of every string).', \"There is a green 'dictionary suffix' arc from each node to the next node in the dictionary that can be reached by following blue arcs.\", 'For example, there is a green arc from (bca) to (a) because (a) is the first node in the dictionary (i.e.', 'a blue node) that is reached when following the blue arcs to (ca) and then on to (a).', 'The green arcs can be computed in linear time by repeatedly traversing blue arcs until a blue node is found, and memoizing this information.'], 'topic': np.str_('nda_violations')}\n"
     ]
    }
   ],
   "source": [
    "query = \"In this example, we will consider a dictionary consisting of the following words: {a, ab, bab, bc, bca, c, caa}. The graph below is the Aho Corasick data structure constructed from the specified dictionary, with each row in the table representing a node in the trie, with the column path indicating the (unique) sequence of characters from the root to the node. The data structure has one node for every prefix of every string in the dictionary. So if (bca) is in the dictionary, then there will be nodes for (bca), (bc), (b), and (). If a node is in the dictionary then it is a blue node. Otherwise it is a grey node. There is a black directed 'child' arc from each node to a node whose name is found by appending one character. So there is a black arc from (bc) to (bca). There is a blue directed 'suffix' arc from each node to the node that is the longest possible strict suffix of it in the graph. For example, for node (caa), its strict suffixes are (aa) and (a) and (). The longest of these that exists in the graph is (a). So there is a blue arc from (caa) to (a). The blue arcs can be computed in linear time by performing a breadth-first search [potential suffix node will always be at lower level] starting from the root. The target for the blue arc of a visited node can be found by following its parent's blue arc to its longest suffix node and searching for a child of the suffix node whose character matches that of the visited node. If the character does not exist as a child, we can find the next longest suffix (following the blue arc again) and then search for the character. We can do this until we either find the character (as child of a node) or we reach the root (which will always be a suffix of every string). There is a green 'dictionary suffix' arc from each node to the next node in the dictionary that can be reached by following blue arcs. For example, there is a green arc from (bca) to (a) because (a) is the first node in the dictionary (i.e. a blue node) that is reached when following the blue arcs to (ca) and then on to (a). The green arcs can be computed in linear time by repeatedly traversing blue arcs until a blue node is found, and memoizing this information. \"\n",
    "result = check_compliance(query, automaton, clf, vectorizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langgraph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"given these COMPLIANCE_RULES = {\n",
    "\n",
    "\"forward_looking_statements\": {\"expect\", \"project\", \"forecast\", \"estimate\", \"anticipate\", \"plan\", \"will\", \"predict\", \"target\", \"could\", \"should\", \"might\", \"potential\", \"projected\"},\n",
    "\n",
    "\"insider_information\": {\"non-public\", \"confidential\", \"undisclosed\", \"insider\", \"privileged\", \"secret\"},\n",
    "\n",
    "\"price_target_inquiry\": {\"price target\", \"future value\", \"next quarter\", \"next year\", \"expected price\", \"forecasted price\"},\n",
    "\n",
    "\"mnpi\": {\"earnings\", \"revenue\", \"profit\", \"loss\", \"financials\", \"guidance\", \"merger\", \"acquisition\", \"divestiture\", \"restructuring\", \"layoffs\", \"contracts\", \"non-public\", \"confidential\", \"undisclosed\", \"not yet announced\"},\n",
    "\n",
    "\"misleading_statements\": {\"always\", \"never\", \"guarantee\", \"risk-free\", \"no risk\", \"promise\", \"certain\", \"assured\"},\n",
    "\n",
    "\"over_promising\": {\"skyrocket\", \"double\", \"triple\", \"explode\", \"massive gains\", \"huge returns\", \"can't lose\", \"win big\"},\n",
    "\n",
    "\"insider_trading_signals\": {\"quiet period\", \"blackout period\", \"trading window\", \"no trading\", \"silent window\"},\n",
    "\n",
    "\"nda_violations\": {\"NDA\", \"non-disclosure agreement\", \"under contract\", \"binding agreement\"},\n",
    "\n",
    "\"conflict_of_interest\": {\"conflict of interest\", \"self-dealing\", \"personal interest\", \"undisclosed relationship\", \"bias\", \"favoritism\"},\n",
    "\n",
    "\"fiduciary_breach\": {\"fiduciary duty\", \"best interest\", \"confidential duty\", \"breach of trust\", \"trustee violation\", \"mismanagement\"},\n",
    "\n",
    "\"high_risk_language\": {\"leverage\", \"high risk\", \"all in\", \"bet\", \"gamble\", \"penny stocks\", \"junk bonds\", \"crypto\", \"unregulated\", \"volatile\"},\n",
    "\n",
    "\"unauthorized_disclosures\": {\"not authorized\", \"not allowed\", \"prohibited\", \"restricted\", \"off-limits\", \"forbidden\"},\n",
    "\n",
    "\"investment_advice_without_disclaimer\": {\"you should invest\", \"buy now\", \"sell now\", \"strong buy\", \"strong sell\", \"must buy\", \"must sell\", \"this stock will\"},\n",
    "\n",
    "\"regulatory_speculation\": {\"SEC will\", \"regulators will\", \"likely fine\", \"expect sanctions\", \"could face penalties\", \"might be banned\", \"regulation changes\"},\n",
    "\n",
    "\"legal_violations\": {\"illegal\", \"against the law\", \"prohibited\", \"unauthorized\", \"unethical\", \"fraud\", \"manipulation\", \"scam\", \"tax evasion\"},\n",
    "\n",
    "\"market_manipulation\": {\"pump and dump\", \"artificially inflate\", \"market making\", \"wash trading\", \"spoofing\", \"cornering the market\", \"price fixing\"},\n",
    "\n",
    "\"unapproved_marketing\": {\"unapproved\", \"not vetted\", \"not reviewed\", \"draft version\", \"preliminary version\", \"for internal use only\"}\n",
    "\n",
    "}\n",
    "\n",
    "is the below compliant or non-compliant?\n",
    "\n",
    "compliance is denoted by not violating the compliance rules above\n",
    "\n",
    "In this example, we will consider a dictionary consisting of the following words: {a, ab, bab, bc, bca, c, caa}. The graph below is the Aho Corasick data structure constructed from the specified dictionary, with each row in the table representing a node in the trie, with the column path indicating the (unique) sequence of characters from the root to the node. The data structure has one node for every prefix of every string in the dictionary. So if (bca) is in the dictionary, then there will be nodes for (bca), (bc), (b), and (). If a node is in the dictionary then it is a blue node. Otherwise it is a grey node. There is a black directed 'child' arc from each node to a node whose name is found by appending one character. So there is a black arc from (bc) to (bca). There is a blue directed 'suffix' arc from each node to the node that is the longest possible strict suffix of it in the graph. For example, for node (caa), its strict suffixes are (aa) and (a) and (). The longest of these that exists in the graph is (a). So there is a blue arc from (caa) to (a). The blue arcs can be computed in linear time by performing a breadth-first search [potential suffix node will always be at lower level] starting from the root. The target for the blue arc of a visited node can be found by following its parent's blue arc to its longest suffix node and searching for a child of the suffix node whose character matches that of the visited node. If the character does not exist as a child, we can find the next longest suffix (following the blue arc again) and then search for the character. We can do this until we either find the character (as child of a node) or we reach the root (which will always be a suffix of every string). There is a green 'dictionary suffix' arc from each node to the next node in the dictionary that can be reached by following blue arcs. For example, there is a green arc from (bca) to (a) because (a) is the first node in the dictionary (i.e. a blue node) that is reached when following the blue arcs to (ca) and then on to (a). The green arcs can be computed in linear time by repeatedly traversing blue arcs until a blue node is found, and memoizing this information.\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      5\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:4891/v1/\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Local Ollama instance\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msk-XXXXXXXXXXXXXXXXXXXX\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m# Include only if required by your server\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a chat completion request\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.1:8b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure the model name is exactly as required\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mgiven these COMPLIANCE_RULES = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward_looking_statements\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforecast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mestimate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manticipate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwill\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcould\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshould\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpotential\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprojected\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsider_information\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnon-public\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfidential\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mundisclosed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsider\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprivileged\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msecret\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprice_target_inquiry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprice target\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfuture value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext quarter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext year\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpected price\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforecasted price\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmnpi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mearnings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevenue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprofit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinancials\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mguidance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43macquisition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdivestiture\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrestructuring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayoffs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontracts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnon-public\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfidential\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mundisclosed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnot yet announced\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmisleading_statements\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malways\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnever\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mguarantee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrisk-free\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno risk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpromise\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcertain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massured\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mover_promising\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskyrocket\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdouble\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtriple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexplode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmassive gains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuge returns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt lose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwin big\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minsider_trading_signals\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquiet period\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblackout period\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrading window\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno trading\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msilent window\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnda_violations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNDA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnon-disclosure agreement\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munder contract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinding agreement\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconflict_of_interest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconflict of interest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mself-dealing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpersonal interest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mundisclosed relationship\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfavoritism\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiduciary_breach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiduciary duty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest interest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfidential duty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbreach of trust\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrustee violation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmismanagement\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhigh_risk_language\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleverage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhigh risk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgamble\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpenny stocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjunk bonds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrypto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munregulated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvolatile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munauthorized_disclosures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnot authorized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnot allowed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprohibited\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrestricted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moff-limits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforbidden\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minvestment_advice_without_disclaimer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myou should invest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbuy now\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msell now\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrong buy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrong sell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmust buy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmust sell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthis stock will\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregulatory_speculation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSEC will\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregulators will\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlikely fine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpect sanctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcould face penalties\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmight be banned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregulation changes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlegal_violations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43millegal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magainst the law\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprohibited\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munauthorized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munethical\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfraud\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmanipulation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtax evasion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmarket_manipulation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpump and dump\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43martificially inflate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmarket making\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwash trading\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspoofing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcornering the market\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprice fixing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m},\u001b[39;49m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munapproved_marketing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munapproved\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnot vetted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnot reviewed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdraft version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreliminary version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfor internal use only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m}\u001b[39;49m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;43m}\u001b[39;49m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;43mis the below compliant or non-compliant?\u001b[39;49m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;43mcompliance is denoted by not violating the compliance rules above\u001b[39;49m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;43mIn this example, we will consider a dictionary consisting of the following words: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43ma, ab, bab, bc, bca, c, caa}. The graph below is the Aho Corasick data structure constructed from the specified dictionary, with each row in the table representing a node in the trie, with the column path indicating the (unique) sequence of characters from the root to the node. The data structure has one node for every prefix of every string in the dictionary. So if (bca) is in the dictionary, then there will be nodes for (bca), (bc), (b), and (). If a node is in the dictionary then it is a blue node. Otherwise it is a grey node. There is a black directed \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchild\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m arc from each node to a node whose name is found by appending one character. So there is a black arc from (bc) to (bca). There is a blue directed \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m arc from each node to the node that is the longest possible strict suffix of it in the graph. For example, for node (caa), its strict suffixes are (aa) and (a) and (). The longest of these that exists in the graph is (a). So there is a blue arc from (caa) to (a). The blue arcs can be computed in linear time by performing a breadth-first search [potential suffix node will always be at lower level] starting from the root. The target for the blue arc of a visited node can be found by following its parent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms blue arc to its longest suffix node and searching for a child of the suffix node whose character matches that of the visited node. If the character does not exist as a child, we can find the next longest suffix (following the blue arc again) and then search for the character. We can do this until we either find the character (as child of a node) or we reach the root (which will always be a suffix of every string). There is a green \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdictionary suffix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m arc from each node to the next node in the dictionary that can be reached by following blue arcs. For example, there is a green arc from (bca) to (a) because (a) is the first node in the dictionary (i.e. a blue node) that is reached when following the blue arcs to (ca) and then on to (a). The green arcs can be computed in linear time by repeatedly traversing blue arcs until a blue node is found, and memoizing this information.\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Print the response from the model\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:704\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    703\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/openai/_base_client.py:1268\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1256\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1264\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1265\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1266\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1267\u001b[0m     )\n\u001b[0;32m-> 1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/openai/_base_client.py:945\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    943\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 945\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/openai/_base_client.py:990\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/openai/_base_client.py:1083\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/openai/_base_client.py:990\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising timeout error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APITimeoutError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/openai/_base_client.py:1083\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/openai/_base_client.py:981\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    978\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 981\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    987\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Documents/github/compliance-v3/venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client for the local model\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:4891/v1/\",  # Local Ollama instance\n",
    "    api_key='sk-XXXXXXXXXXXXXXXXXXXX'   # Include only if required by your server\n",
    ")\n",
    "\n",
    "# Create a chat completion request\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama3.1:8b\",  # Ensure the model name is exactly as required\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print the response from the model\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain the importance of fast language models\"}\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, client: Groq, system: str = \"\") -> None:\n",
    "        self.client = client\n",
    "        self.system = system\n",
    "        self.messages: list = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def __call__(self, message=\"\"):\n",
    "        if message:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\", messages=self.messages\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "get_planet_mass:\n",
    "e.g. get_planet_mass: Earth\n",
    "returns weight of the planet in kg\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What is the mass of Earth times 2?\n",
    "Thought: I need to find the mass of Earth\n",
    "Action: get_planet_mass: Earth\n",
    "PAUSE \n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: 5.972e24\n",
    "\n",
    "Thought: I need to multiply this by 2\n",
    "Action: calculate: 5.972e24 * 2\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this: \n",
    "\n",
    "Observation: 1,1944×10e25\n",
    "\n",
    "If you have the answer, output it as the Answer.\n",
    "\n",
    "Answer: The mass of Earth times 2 is 1,1944×10e25.\n",
    "\n",
    "Now it's your turn:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def calculate(operation: str) -> float:\n",
    "    return eval(operation)\n",
    "\n",
    "\n",
    "def get_planet_mass(planet) -> float:\n",
    "    match planet.lower():\n",
    "        case \"earth\":\n",
    "            return 5.972e24\n",
    "        case \"jupiter\":\n",
    "            return 1.898e27\n",
    "        case \"mars\":\n",
    "            return 6.39e23\n",
    "        case \"mercury\":\n",
    "            return 3.285e23\n",
    "        case \"neptune\":\n",
    "            return 1.024e26\n",
    "        case \"saturn\":\n",
    "            return 5.683e26\n",
    "        case \"uranus\":\n",
    "            return 8.681e25\n",
    "        case \"venus\":\n",
    "            return 4.867e24\n",
    "        case _:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neil_tyson = Agent(client=client, system=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = neil_tyson(\"What is the mass of Mercury times 5?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = neil_tyson()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_planet_mass(\"mercury\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_prompt = \"Observation: {}\".format(result)\n",
    "next_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = neil_tyson(next_prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = neil_tyson(next_prompt)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = calculate(\"3.285e23 * 5\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_prompt = \"Observation: {}\".format(result)\n",
    "next_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = neil_tyson(next_prompt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in neil_tyson.messages:\n",
    "    print(msg['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use Agent Cycle\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def loop(max_iterations=10, query: str = \"\"):\n",
    "\n",
    "    agent = Agent(client=client, system=system_prompt)\n",
    "\n",
    "    tools = [\"calculate\", \"get_planet_mass\"]\n",
    "\n",
    "    next_prompt = query\n",
    "\n",
    "    i = 0\n",
    "  \n",
    "    while i < max_iterations:\n",
    "        i += 1\n",
    "        result = agent(next_prompt)\n",
    "        print(result)\n",
    "\n",
    "        if \"PAUSE\" in result and \"Action\" in result:\n",
    "            action = re.findall(r\"Action: ([a-z_]+): (.+)\", result, re.IGNORECASE)\n",
    "            chosen_tool = action[0][0]\n",
    "            arg = action[0][1]\n",
    "\n",
    "            if chosen_tool in tools:\n",
    "                result_tool = eval(f\"{chosen_tool}('{arg}')\")\n",
    "                next_prompt = f\"Observation: {result_tool}\"\n",
    "\n",
    "            else:\n",
    "                next_prompt = \"Observation: Tool not found\"\n",
    "\n",
    "            print(next_prompt)\n",
    "            continue\n",
    "\n",
    "        if \"Answer\" in result:\n",
    "            break\n",
    "\n",
    "\n",
    "loop(query=\"What is the mass of Earth plus the mass of Saturn and all of that times 2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUT /regex_rules_index\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"custom_standard_analyzer\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\"lowercase\", \"stop\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"sanitization_term\": {\n",
    "        \"type\": \"text\",\n",
    "        \"fields\": {\n",
    "          \"standard\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"custom_standard_analyzer\"\n",
    "          },\n",
    "          \"raw\": {\n",
    "            \"type\": \"keyword\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"entity\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"standard\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compliance_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
